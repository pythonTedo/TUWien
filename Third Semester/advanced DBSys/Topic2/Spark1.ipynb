{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notebook 1 - Basic concepts of Spark. \n",
    "\n",
    "This is a Scala notebook. To run any commands you first have to create a SparkContext."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you connect to our cluster, then SparkContext is already loaded as variable sc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Basic RDD Operations\n",
    "\n",
    "**Create a RDD**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First create a collection of objects, for example a list of integers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val x = List(1,2,2,3,4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then you create an RDD from it. Here, we use \"ParallelCollection\" RDD, used by Spark to execute queries on it in parallel . "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val rdd = sc.parallelize(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**RDD Transformations**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val result = rdd.map(x => (x,x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result.collect().foreach(x => print(x + \" \"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Note that Scala is lazily evaluated, so we need to use \"collect()\" to actually produce the result._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val result = rdd.flatMap(x => x.to(3))\n",
    "result.collect().foreach(x => print(x + \" \"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rdd.filter(x => x!=2).collect().foreach(x => print(x + \" \"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rdd.map(x => x*x).flatMap(x => x.to(5)).filter(x => x!=3).collect().foreach(x => print(x + \" \"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**RDD Actions (Reduce)**\n",
    "\n",
    "Other actions, for example, are reduce() and map(). For more informations on supported RDD operations, check the documentation: https://spark.apache.org/docs/latest/rdd-programming-guide.html#rdd-operations. Please study the documentation, as it's expected that you are able to use these basic RDD actions to solve the exercises. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val sum = rdd.reduce(((x,y)=>x+y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val RDDNumbers= sc.parallelize((1).to(10))\n",
    "val RDDsquared = RDDNumbers.map(x => x*x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The RDD will be loaded to memory and the map transformation will be computed as soon as an action is called."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "RDDsquared.reduce((x,y) => x+y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For key-value pairs you can use the action reduceByKey()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Write your first MapReduce Job!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The standard example for MapReduce is WordCount. We use as input the diamonds dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**1. Load the dataset as an RDD to SparkContext (sc)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val diamonds = sc.textFile(\"/home/adbs22/shared/diamonds.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use 'take' here to reduce the input to the first 20 elements:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "diamonds.collect().take(20).foreach(x => println(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val entries = diamonds.flatMap(line => line.split(\",\"))\n",
    "entries.take(30).foreach(x => println(x))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "entries.collect().take(20).foreach(x => println(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2. Create the MapReduce WordCount procedure**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the map procedure key-value pairs are created. Spark provides the reducebyKey method to easily aggregate over the key-value pairs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val wordcounts = entries.map (word => (word, 1)).\n",
    "reduceByKey( (x,y)  => x+y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "var r = sc.parallelize(List(1,2,3,4,5),2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "var r2 = r.takeSample(true,7,2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The entire code for the Mapreduce wordcount procedure in Scala: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val ex1= sc.parallelize (List(1,2,3,4,5,6), 2)  // 2nd arg: no. partitions\n",
    "val ex2= sc.parallelize (Array (\"Alice\", \"Bob\", \"Caroline\"))\n",
    "val ex3= sc.parallelize (1 to 100)\n",
    "println(ex3.getNumPartitions)   \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### For more information see: https://spark.apache.org/docs/latest/rdd-programming-guide.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# More basic RDD functions: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import util.Random.nextInt\n",
    "val randomList = sc.parallelize(List.fill(40)(util.Random.nextInt))   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  Compute the square root of each element in the list 'randomList', and then produce the sum of all square roots which are larger than 3:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scala.math.sqrt  // enables you to use the function sqrt() for computing the square root of a number\n",
    "randomList.map(x => sqrt(x)).filter(x => x > 3).reduce(((x,y)=>x+y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  Produce the minimal value of 'randomList'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val minimal = randomList.reduce( (x,y) => x.min(y) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  Produce for the 'randomList' the average absolute difference to the minimal value. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val averageDifference = randomList.map(x => (x - minimal).abs  ).reduce(_+_) / randomList.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  You are given a random list of words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val words = sc.parallelize(List(\"key\", \"data\", \"car\", \"fish\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  Consider the following two predefined functions on String."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getTails (word:String) =  word.tails.toList.filter(x => x.length > 0)\n",
    "getTails(\"Example\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getInitials (word:String) = word.inits.toList.filter(x => x.length > 0)\n",
    "getInitials(\"Example\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  Using the functions above, produce a single list containing all substrings of elements from the list 'words'. A formal definition of 'substring', is given here: https://en.wikipedia.org/wiki/Substring#Substring."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "hide-cell"
    ]
   },
   "outputs": [],
   "source": [
    "def allSubstrings = words.flatMap(getInitials).flatMap(getTails)\n",
    "allSubstrings.collect()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "spylon-kernel",
   "language": "scala",
   "name": "spylon-kernel"
  },
  "language_info": {
   "codemirror_mode": "text/x-scala",
   "file_extension": ".scala",
   "help_links": [
    {
     "text": "MetaKernel Magics",
     "url": "https://metakernel.readthedocs.io/en/latest/source/README.html"
    }
   ],
   "mimetype": "text/x-scala",
   "name": "scala",
   "pygments_lexer": "scala",
   "version": "0.4.1"
  },
  "toc-autonumbering": false,
  "toc-showmarkdowntxt": true,
  "toc-showtags": true
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
