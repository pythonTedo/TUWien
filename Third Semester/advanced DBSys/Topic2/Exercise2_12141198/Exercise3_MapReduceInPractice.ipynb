{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Exercise 3 (MapReduce in Practice)   &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;     [4 points]\n",
    "---\n",
    "\n",
    "For this exercise, you are tasked with writing your own Hadoop MapReduce program in Python and to\n",
    "run it on the cluster on the provided datasets.   \n",
    "You may look at the exercise sheet for all the information on the datasets and this task.\n",
    "\n",
    "\n",
    "**Note:** *When accessing files in the HDFS, you need to prepend “hdfs://” to the address string. For\n",
    "quick testing of solutions, there are smaller versions of large datasets (> 1 GB) on the local file-system,\n",
    "ending with “_small.csv”. Make sure that your MapReduce job also works on the complete dataset\n",
    "on the cluster.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Saving variables to access the file locations\n",
    "articles_hdfs='hdfs:///user/adbs23_shared/hm/articles.csv'\n",
    "articles='/home/adbs23/adbs23_shared/hm/articles.csv'\n",
    "\n",
    "customers_hdfs='hdfs:///user/adbs23_shared/hm/customers.csv'\n",
    "customers='/home/adbs23/adbs23_shared/hm/customers.csv'\n",
    "\n",
    "transactions_hdfs='hdfs:///user/adbs23_shared/hm/transactions.csv'\n",
    "transactions='/home/adbs23/adbs23_shared/hm/transactions.csv'\n",
    "\n",
    "transactions_small_hdfs='hdfs:///user/adbs23_shared/hm/transactions_small.csv'\n",
    "transactions_small ='/home/adbs23/adbs23_shared/hm/transactions_small.csv'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "- ### **a) Write a MapReduce job with “articles.csv” as input and following output:**  \n",
    "\n",
    "For each garment group, show the most frequent product, the second most frequent section and the most frequent department it appears inside the article.csv file; make sure output has the following schema:\n",
    "\n",
    "            garment_group_name, prod_name, section_name,  department_name\n",
    "\n",
    "The product names are stored in \"prod_name\", the deparment name in \"department_name\", the garment group in \"garment_group_name\" and the section in \"section_name\". In case that there are multiple departments, garment groups or sections with the same number of occurences, you may resolve these conflicts randomly, i.e. pick one of them arbitrarily. In case there is only one section, or all sections appear with the same frequency, just pick the most frequent one, and resolve conflicts randomly. \n",
    "\n",
    "Make sure that your program correctly deals with the header, and possible sparse values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting mymrjob1.py\n"
     ]
    }
   ],
   "source": [
    "%%file mymrjob1.py\n",
    "\n",
    "# This will create a local file to run your MapReduce program  \n",
    "\n",
    "from mrjob.job import MRJob\n",
    "from mrjob.step import MRStep\n",
    "from mrjob.util import log_to_stream, log_to_null\n",
    "from mr3px.csvprotocol import CsvProtocol\n",
    "import csv \n",
    "import operator\n",
    "import logging\n",
    "from collections import defaultdict\n",
    "\n",
    "log = logging.getLogger(__name__)\n",
    "# \n",
    "#  Below is the skeleton for a MapReduce program in mrjob.\n",
    "#  Write your own solution here. Be sure that it actually runs successfully.\n",
    "\n",
    "class MyMRJob1(MRJob):\n",
    "    \n",
    "    \n",
    "    OUTPUT_PROTOCOL = CsvProtocol  # write output as CSV\n",
    "    \n",
    "    def set_up_logging(cls, quiet=False, verbose=False, stream=None):  \n",
    "        log_to_stream(name='mrjob', debug=verbose, stream=stream)\n",
    "        log_to_stream(name='__main__', debug=verbose, stream=stream)\n",
    "\n",
    "    def mapper_prodcount(self, _, line):\n",
    "        # TODO\n",
    "        row = next(csv.reader([line]))\n",
    "        if row[0] == \"product_id\":\n",
    "            return  # skip header row\n",
    "        garment_group = row[23]\n",
    "        \n",
    "        prod_name = row[2]\n",
    "        section_name = row[21]\n",
    "        department_name = row[15]\n",
    "        yield garment_group, (prod_name, section_name, department_name)\n",
    "        \n",
    "# use of a combiner is optional. It may speed up your job. Be sure that using the combiner preserves the correctness. \n",
    "#     def combiner_mrjob1(self,key,valuelist):\n",
    "        #TODO\n",
    "        \n",
    "     # The reducer now creates a dict for all department_names, product_names and sections\n",
    "     # and in the end returns the most or second most frequent values based on its contents\n",
    "    def reducer_prodcount(self,key,pairs):\n",
    "        # TODO\n",
    "        product_counts = defaultdict(int)\n",
    "        section_counts = defaultdict(int)\n",
    "        department_counts = defaultdict(int)\n",
    "        for prod_name, section_name, department_name in pairs:\n",
    "            product_counts[prod_name] += 1\n",
    "            section_counts[section_name] += 1\n",
    "            department_counts[department_name] += 1\n",
    "        # get most frequent product\n",
    "        most_freq_product = max(product_counts, key=product_counts.get)\n",
    "        # get second most frequent section\n",
    "        freq_sections = sorted(section_counts, key=section_counts.get, reverse=True)\n",
    "        if len(freq_sections) == 1:\n",
    "            second_most_freq_section = freq_sections[0]\n",
    "        elif section_counts[freq_sections[0]] != section_counts[freq_sections[1]]:\n",
    "            second_most_freq_section = freq_sections[1]\n",
    "        else:\n",
    "            second_most_freq_section = freq_sections[0] if hash(freq_sections[0]) < hash(freq_sections[1]) else freq_sections[1]\n",
    "        # get most frequent department\n",
    "        most_freq_department = max(department_counts, key=department_counts.get)\n",
    "        # yield output as a list\n",
    "        \n",
    "        #log.warning(var)\n",
    "        yield key, (key, most_freq_product, second_most_freq_section, most_freq_department)\n",
    "        \n",
    "    \n",
    "    def steps(self):\n",
    "        return [\n",
    "            MRStep(mapper=self.mapper_prodcount,\n",
    "                   reducer=self.reducer_prodcount)            \n",
    "        ]\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    MyMRJob1.run()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Running a local MRjob "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using configs in /etc/mrjob.conf\n",
      "No configs specified for inline runner\n",
      "Creating temp directory /tmp/mymrjob1.e12141198.20230505.160205.901491\n",
      "Running step 1 of 1...\n",
      "job output is in /tmp/mymrjob1.e12141198.20230505.160205.901491/output\n",
      "Streaming final output from /tmp/mymrjob1.e12141198.20230505.160205.901491/output...\n",
      "Removing temp directory /tmp/mymrjob1.e12141198.20230505.160205.901491...\n"
     ]
    }
   ],
   "source": [
    "!python3 mymrjob1.py $articles > output1.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Running a Hadoop job"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using configs in /etc/mrjob.conf\n",
      "Looking for hadoop binary in $PATH...\n",
      "Found hadoop binary: /bin/hadoop\n",
      "STDOUT: #\n",
      "STDOUT: # There is insufficient memory for the Java Runtime Environment to continue.\n",
      "STDOUT: # Native memory allocation (mmap) failed to map 429391872 bytes for committing reserved memory.\n",
      "STDOUT: # An error report file with more information is saved as:\n",
      "STDOUT: # /home/adbs23/e12141198/hs_err_pid2143763.log\n",
      "STDERR: OpenJDK 64-Bit Server VM warning: INFO: os::commit_memory(0x000000055a800000, 429391872, 0) failed; error='Cannot allocate memory' (errno=12)\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/adbs23/e12141198/.local/lib/python3.6/site-packages/mrjob/fs/hadoop.py\", line 310, in exists\n",
      "    ok_stderr=[_HADOOP_LS_NO_SUCH_FILE])\n",
      "  File \"/home/adbs23/e12141198/.local/lib/python3.6/site-packages/mrjob/fs/hadoop.py\", line 183, in invoke_hadoop\n",
      "    raise CalledProcessError(proc.returncode, args)\n",
      "subprocess.CalledProcessError: Command '['/bin/hadoop', 'fs', '-ls', 'hdfs:///user/adbs23_shared/hm/articles.csv']' returned non-zero exit status 1.\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"mymrjob1.py\", line 77, in <module>\n",
      "    MyMRJob1.run()\n",
      "  File \"/home/adbs23/e12141198/.local/lib/python3.6/site-packages/mrjob/job.py\", line 616, in run\n",
      "    cls().execute()\n",
      "  File \"/home/adbs23/e12141198/.local/lib/python3.6/site-packages/mrjob/job.py\", line 687, in execute\n",
      "    self.run_job()\n",
      "  File \"/home/adbs23/e12141198/.local/lib/python3.6/site-packages/mrjob/job.py\", line 636, in run_job\n",
      "    runner.run()\n",
      "  File \"/home/adbs23/e12141198/.local/lib/python3.6/site-packages/mrjob/runner.py\", line 500, in run\n",
      "    self._check_input_paths()\n",
      "  File \"/home/adbs23/e12141198/.local/lib/python3.6/site-packages/mrjob/runner.py\", line 1133, in _check_input_paths\n",
      "    self._check_input_path(path)\n",
      "  File \"/home/adbs23/e12141198/.local/lib/python3.6/site-packages/mrjob/runner.py\", line 1145, in _check_input_path\n",
      "    if not self.fs.exists(path):\n",
      "  File \"/home/adbs23/e12141198/.local/lib/python3.6/site-packages/mrjob/fs/composite.py\", line 142, in exists\n",
      "    return self._do('exists', path_glob)\n",
      "  File \"/home/adbs23/e12141198/.local/lib/python3.6/site-packages/mrjob/fs/composite.py\", line 124, in _do\n",
      "    return self._handle(name, path, path)\n",
      "  File \"/home/adbs23/e12141198/.local/lib/python3.6/site-packages/mrjob/fs/composite.py\", line 110, in _handle\n",
      "    return getattr(fs, name)(*args, **kwargs)\n",
      "  File \"/home/adbs23/e12141198/.local/lib/python3.6/site-packages/mrjob/fs/hadoop.py\", line 314, in exists\n",
      "    raise IOError(\"Could not check path %s\" % path_glob)\n",
      "OSError: Could not check path hdfs:///user/adbs23_shared/hm/articles.csv\n"
     ]
    }
   ],
   "source": [
    "!python3 mymrjob1.py -r hadoop --hadoop-streaming-jar \"/usr/lib/hadoop/tools/lib/hadoop-streaming-3.3.4.jar\" $articles_hdfs > output.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "---\n",
    "\n",
    "- ### **b) Write a MapReduce job with all three datasets as input and following output:**  \n",
    "For all customers older than 25 years, show the number of transactions items they were involved in with articles from department with name 'Jacket' or 'Woven'. \n",
    "\n",
    "\n",
    "Make sure to have the following format in your final output:\n",
    "\n",
    "            customer_id,count_transactions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting mymrjob2.py\n"
     ]
    }
   ],
   "source": [
    "%%file mymrjob2.py\n",
    "# This will create a local file to run your MapReduce program  \n",
    "\n",
    "from mrjob.job import MRJob\n",
    "from mrjob.step import MRStep\n",
    "from mrjob.util import log_to_stream, log_to_null\n",
    "from mr3px.csvprotocol import CsvProtocol\n",
    "import csv \n",
    "import logging\n",
    "\n",
    "log = logging.getLogger(__name__)\n",
    "# \n",
    "#  Below is the skeleton for a MapReduce program in mrjob.\n",
    "#  Write your own solution here. Be sure that it actually runs successfully.\n",
    "class MyMRJob2(MRJob):\n",
    "    \n",
    "    OUTPUT_PROTOCOL = CsvProtocol  # write output as CSV\n",
    "    def set_up_logging(cls, quiet=False, verbose=False, stream=None):  \n",
    "        log_to_stream(name='mrjob', debug=verbose, stream=stream)\n",
    "        log_to_stream(name='__main__', debug=verbose, stream=stream)\n",
    "\n",
    "#   Feel free to rename the functions\n",
    "    def mapper_mrjob2(self, _, line):\n",
    "        if not line.startswith(\"custom\") and not line.startswith(\"article\") and not line.startswith(\"t_dat\"):   \n",
    "            column_entries = line.split(\",\")\n",
    "            if len(column_entries) >= 25 and (column_entries[15] == \"Jacket\" or column_entries[15] == \"Woven\"):\n",
    "                yield column_entries[0], \"article\"      \n",
    "            #    yield \"article\", column_entries[0]\n",
    "            if len(column_entries) == 5:\n",
    "                article_num_with_0 = \"0\" + column_entries[2]\n",
    "                yield column_entries[2], [\"transaction\", column_entries[1]] \n",
    "                yield column_entries[1], \"transaction\"\n",
    "                #yield \"transaction\", [column_entries[1], column_entries[2]]\n",
    "            if len(column_entries) == 7:\n",
    "                if not column_entries[5] == '':\n",
    "                    if int(column_entries[5]) > 25:\n",
    "                        yield column_entries[0], \"customer\"\n",
    "        \n",
    "# use of a combiner is optional. It may speed up your job. Be sure that using the combiner preserves the correctness. \n",
    "#     def combiner_mrjob2(self,key,valuelist):\n",
    "        #TODO\n",
    "        \n",
    "    def reducer1_mrjob2(self,key,valuelist):\n",
    "        valList = list(valuelist)\n",
    "        #yield key, valList\n",
    "        if len(valList) > 1:\n",
    "            if valList[0] == \"article\" and valList[1][0] == \"transaction\":\n",
    "                for i in range (len(valList) - 1):\n",
    "                    yield valList[i + 1][1], [1, \"article\"]\n",
    "               # yield key, valList\n",
    "            if valList[0] == \"transaction\" and valList[(len(valList) - 1)] == \"customer\":\n",
    "                yield key, [0, \"customer\"]\n",
    "                 \n",
    "    def reducer2_mrjob2(self,key,valuelist):\n",
    "        valList = list(valuelist)\n",
    "        #yield None, [key, valList]\n",
    "        \n",
    "        sum_of_vars = 0\n",
    "        useable_customer = False\n",
    "        useable_article = False\n",
    "        for i in range (len(valList)):\n",
    "            sum_of_vars = sum_of_vars + 1\n",
    "            if valList[i][1] == \"customer\":\n",
    "                useable_customer = True\n",
    "                sum_of_vars = sum_of_vars - 1\n",
    "            if valList[i][1] == \"article\":\n",
    "                useable_article = True\n",
    "        if useable_customer and useable_article:\n",
    "            yield key, [key, sum_of_vars ]\n",
    "        \n",
    "        \n",
    "    def steps(self):\n",
    "        return [ \n",
    "            MRStep(\n",
    "            mapper=self.mapper_mrjob2, \n",
    "#             combiner=self.combiner_mrjob1, \n",
    "            reducer=self.reducer1_mrjob2\n",
    "            ),\n",
    "            MRStep(reducer=self.reducer2_mrjob2\n",
    "            )\n",
    "        ]\n",
    "if __name__ == '__main__':\n",
    "    MyMRJob2.run()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using configs in /etc/mrjob.conf\n",
      "No configs specified for inline runner\n",
      "Creating temp directory /tmp/mymrjob2.e12141198.20230505.155914.527368\n",
      "Running step 1 of 2...\n",
      "Running step 2 of 2...\n",
      "job output is in /tmp/mymrjob2.e12141198.20230505.155914.527368/output\n",
      "Streaming final output from /tmp/mymrjob2.e12141198.20230505.155914.527368/output...\n",
      "Removing temp directory /tmp/mymrjob2.e12141198.20230505.155914.527368...\n"
     ]
    }
   ],
   "source": [
    "!python3 mymrjob2.py  $articles $transactions_small $customers > output2.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using configs in /etc/mrjob.conf\n",
      "Looking for hadoop binary in $PATH...\n",
      "Found hadoop binary: /bin/hadoop\n",
      "STDOUT: #\n",
      "STDOUT: # There is insufficient memory for the Java Runtime Environment to continue.\n",
      "STDOUT: # Native memory allocation (mmap) failed to map 429391872 bytes for committing reserved memory.\n",
      "STDOUT: # An error report file with more information is saved as:\n",
      "STDOUT: # /home/adbs23/e12141198/hs_err_pid2143411.log\n",
      "STDERR: OpenJDK 64-Bit Server VM warning: INFO: os::commit_memory(0x000000055a800000, 429391872, 0) failed; error='Cannot allocate memory' (errno=12)\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/adbs23/e12141198/.local/lib/python3.6/site-packages/mrjob/fs/hadoop.py\", line 310, in exists\n",
      "    ok_stderr=[_HADOOP_LS_NO_SUCH_FILE])\n",
      "  File \"/home/adbs23/e12141198/.local/lib/python3.6/site-packages/mrjob/fs/hadoop.py\", line 183, in invoke_hadoop\n",
      "    raise CalledProcessError(proc.returncode, args)\n",
      "subprocess.CalledProcessError: Command '['/bin/hadoop', 'fs', '-ls', 'hdfs:///user/adbs23_shared/hm/articles.csv']' returned non-zero exit status 1.\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"mymrjob2.py\", line 82, in <module>\n",
      "    MyMRJob2.run()\n",
      "  File \"/home/adbs23/e12141198/.local/lib/python3.6/site-packages/mrjob/job.py\", line 616, in run\n",
      "    cls().execute()\n",
      "  File \"/home/adbs23/e12141198/.local/lib/python3.6/site-packages/mrjob/job.py\", line 687, in execute\n",
      "    self.run_job()\n",
      "  File \"/home/adbs23/e12141198/.local/lib/python3.6/site-packages/mrjob/job.py\", line 636, in run_job\n",
      "    runner.run()\n",
      "  File \"/home/adbs23/e12141198/.local/lib/python3.6/site-packages/mrjob/runner.py\", line 500, in run\n",
      "    self._check_input_paths()\n",
      "  File \"/home/adbs23/e12141198/.local/lib/python3.6/site-packages/mrjob/runner.py\", line 1133, in _check_input_paths\n",
      "    self._check_input_path(path)\n",
      "  File \"/home/adbs23/e12141198/.local/lib/python3.6/site-packages/mrjob/runner.py\", line 1145, in _check_input_path\n",
      "    if not self.fs.exists(path):\n",
      "  File \"/home/adbs23/e12141198/.local/lib/python3.6/site-packages/mrjob/fs/composite.py\", line 142, in exists\n",
      "    return self._do('exists', path_glob)\n",
      "  File \"/home/adbs23/e12141198/.local/lib/python3.6/site-packages/mrjob/fs/composite.py\", line 124, in _do\n",
      "    return self._handle(name, path, path)\n",
      "  File \"/home/adbs23/e12141198/.local/lib/python3.6/site-packages/mrjob/fs/composite.py\", line 110, in _handle\n",
      "    return getattr(fs, name)(*args, **kwargs)\n",
      "  File \"/home/adbs23/e12141198/.local/lib/python3.6/site-packages/mrjob/fs/hadoop.py\", line 314, in exists\n",
      "    raise IOError(\"Could not check path %s\" % path_glob)\n",
      "OSError: Could not check path hdfs:///user/adbs23_shared/hm/articles.csv\n"
     ]
    }
   ],
   "source": [
    "!python3 mymrjob2.py -r hadoop --hadoop-streaming-jar \"/usr/lib/hadoop/tools/lib/hadoop-streaming-3.3.4.jar\" $articles_hdfs $transactions_hdfs $customers_hdfs > output2.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "---\n",
    "\n",
    "- ### **c) Once your jobs have run successfully on the cluster, use the provided commands in the notebook to look up the counters of your Mapreduce job(s).**  \n",
    "Alternatively, you can also read the counters from the output cells above, after a job has succesfully run on the cluster.\n",
    "Use the counters to determine for each job what the replication rate was, as well as the input and output size. Note: for this you will need to determine the job ids. These are shown in the output when running a job.  \n",
    "**Note:** _Be sure to replace the dummy job ID below with the real one you get after running it on the cluster!_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-05-05 18:01:31,286 INFO client.DefaultNoHARMFailoverProxyProvider: Connecting to ResourceManager at captain01.os.hpc.tuwien.ac.at/192.168.88.133:8032\n",
      "2023-05-05 18:01:32,474 INFO mapred.ClientServiceDelegate: Could not get Job info from RM for job job_1681716720238_0466. Redirecting to job history server.\n",
      "2023-05-05 18:01:32,533 INFO tools.CLI: Could not obtain job info after 1 attempt(s). Sleeping for 2 seconds and retrying.\n",
      "2023-05-05 18:01:34,537 INFO mapred.ClientServiceDelegate: Could not get Job info from RM for job job_1681716720238_0466. Redirecting to job history server.\n",
      "2023-05-05 18:01:34,580 INFO tools.CLI: Could not obtain job info after 2 attempt(s). Sleeping for 2 seconds and retrying.\n",
      "2023-05-05 18:01:36,584 INFO mapred.ClientServiceDelegate: Could not get Job info from RM for job job_1681716720238_0466. Redirecting to job history server.\n",
      "2023-05-05 18:01:36,627 INFO tools.CLI: Could not obtain job info after 3 attempt(s). Sleeping for 2 seconds and retrying.\n",
      "2023-05-05 18:01:38,631 INFO mapred.ClientServiceDelegate: Could not get Job info from RM for job job_1681716720238_0466. Redirecting to job history server.\n",
      "Could not find job job_1681716720238_0466\n"
     ]
    }
   ],
   "source": [
    "job_id = \"job_1681716720238_0466\"\n",
    "!mapred job -counter $job_id org.apache.hadoop.mapreduce.TaskCounter MAP_INPUT_RECORDS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-05-05 18:01:40,193 INFO client.DefaultNoHARMFailoverProxyProvider: Connecting to ResourceManager at captain01.os.hpc.tuwien.ac.at/192.168.88.133:8032\n",
      "2023-05-05 18:01:41,326 INFO mapred.ClientServiceDelegate: Could not get Job info from RM for job job_1681716720238_0466. Redirecting to job history server.\n",
      "2023-05-05 18:01:41,379 INFO tools.CLI: Could not obtain job info after 1 attempt(s). Sleeping for 2 seconds and retrying.\n",
      "2023-05-05 18:01:43,384 INFO mapred.ClientServiceDelegate: Could not get Job info from RM for job job_1681716720238_0466. Redirecting to job history server.\n",
      "2023-05-05 18:01:43,431 INFO tools.CLI: Could not obtain job info after 2 attempt(s). Sleeping for 2 seconds and retrying.\n",
      "2023-05-05 18:01:45,435 INFO mapred.ClientServiceDelegate: Could not get Job info from RM for job job_1681716720238_0466. Redirecting to job history server.\n",
      "2023-05-05 18:01:45,479 INFO tools.CLI: Could not obtain job info after 3 attempt(s). Sleeping for 2 seconds and retrying.\n",
      "2023-05-05 18:01:47,483 INFO mapred.ClientServiceDelegate: Could not get Job info from RM for job job_1681716720238_0466. Redirecting to job history server.\n",
      "Could not find job job_1681716720238_0466\n"
     ]
    }
   ],
   "source": [
    "!mapred job -counter $job_id org.apache.hadoop.mapreduce.TaskCounter MAP_OUTPUT_RECORDS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-05-05 18:01:49,046 INFO client.DefaultNoHARMFailoverProxyProvider: Connecting to ResourceManager at captain01.os.hpc.tuwien.ac.at/192.168.88.133:8032\n",
      "2023-05-05 18:01:50,195 INFO mapred.ClientServiceDelegate: Could not get Job info from RM for job job_1681716720238_0466. Redirecting to job history server.\n",
      "2023-05-05 18:01:50,246 INFO tools.CLI: Could not obtain job info after 1 attempt(s). Sleeping for 2 seconds and retrying.\n",
      "2023-05-05 18:01:52,251 INFO mapred.ClientServiceDelegate: Could not get Job info from RM for job job_1681716720238_0466. Redirecting to job history server.\n",
      "2023-05-05 18:01:52,295 INFO tools.CLI: Could not obtain job info after 2 attempt(s). Sleeping for 2 seconds and retrying.\n",
      "2023-05-05 18:01:54,298 INFO mapred.ClientServiceDelegate: Could not get Job info from RM for job job_1681716720238_0466. Redirecting to job history server.\n",
      "2023-05-05 18:01:54,348 INFO tools.CLI: Could not obtain job info after 3 attempt(s). Sleeping for 2 seconds and retrying.\n",
      "2023-05-05 18:01:56,351 INFO mapred.ClientServiceDelegate: Could not get Job info from RM for job job_1681716720238_0466. Redirecting to job history server.\n",
      "Could not find job job_1681716720238_0466\n"
     ]
    }
   ],
   "source": [
    "!mapred job -counter $job_id org.apache.hadoop.mapreduce.lib.input.FileInputFormatCounter BYTES_READ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-05-05 18:01:57,912 INFO client.DefaultNoHARMFailoverProxyProvider: Connecting to ResourceManager at captain01.os.hpc.tuwien.ac.at/192.168.88.133:8032\n",
      "2023-05-05 18:01:58,998 INFO mapred.ClientServiceDelegate: Could not get Job info from RM for job job_1681716720238_0466. Redirecting to job history server.\n",
      "2023-05-05 18:01:59,066 INFO tools.CLI: Could not obtain job info after 1 attempt(s). Sleeping for 2 seconds and retrying.\n",
      "2023-05-05 18:02:01,071 INFO mapred.ClientServiceDelegate: Could not get Job info from RM for job job_1681716720238_0466. Redirecting to job history server.\n",
      "2023-05-05 18:02:01,119 INFO tools.CLI: Could not obtain job info after 2 attempt(s). Sleeping for 2 seconds and retrying.\n",
      "2023-05-05 18:02:03,123 INFO mapred.ClientServiceDelegate: Could not get Job info from RM for job job_1681716720238_0466. Redirecting to job history server.\n",
      "2023-05-05 18:02:03,180 INFO tools.CLI: Could not obtain job info after 3 attempt(s). Sleeping for 2 seconds and retrying.\n",
      "2023-05-05 18:02:05,184 INFO mapred.ClientServiceDelegate: Could not get Job info from RM for job job_1681716720238_0466. Redirecting to job history server.\n",
      "Could not find job job_1681716720238_0466\n"
     ]
    }
   ],
   "source": [
    "!mapred job -counter $job_id org.apache.hadoop.mapreduce.lib.output.FileOutputFormatCounter BYTES_WRITTEN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Replication Rates and Input and Output Sizes:\n",
    "\n",
    "* Replication Rate for Task 1. :   MAP_INPUT_RECORDS / MAP_OUTPUT_RECODRS\n",
    "* Communication Cost:   MAP_OUTPUT_RECORDS  (Or the size of the output in bytes, either works)\n",
    "*  Input Size for Task 1. :  BYTES_READ\n",
    "* Output Size for Task 1. : BYTES_READ"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: if your job had multiple steps, just state the replication rates for each step. Make sure to compute the cummulative costs for the other measures, though.\n",
    "* Replication Rate for Task 2. :  AS ABOVE (but for each round)\n",
    "* Communication Cost:  MAP_OUTPUT_RECORDS (of each job) plus the BYTES_READ (or MAP_INPUT_RECORDS) of the second job\n",
    "* Input Size for Task 2. :  AS ABOVE\n",
    "* Output Size for Task 2. : AS ABOVE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For more counters, you can check the documentation, under https://hadoop.apache.org/docs/current/api/org/apache/hadoop/mapreduce/TaskCounter and https://hadoop.apache.org/docs/current/api/org/apache/hadoop/mapreduce/lib/output/FileOutputFormatCounter.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## **Your solution for Exercise 3 will consist of:**  \n",
    "*  This notebook, filled with your solution, including the information on the replication rate, and the input and output sizes. \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ADBS23)",
   "language": "python",
   "name": "python3_adbs23"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
