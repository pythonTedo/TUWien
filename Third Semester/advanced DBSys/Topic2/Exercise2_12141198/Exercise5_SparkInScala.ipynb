{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# Exercise 5 (Spark in Scala)   &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;     [4 points]\n",
    "---\n",
    "\n",
    "For this exercise, you will work on this JupyterLab notebook, and solve the tasks listed herein. These tasks, in addition to writing Spark code, require you to analyse various query plans and to reason about them.\n",
    "\n",
    "To familiarise yourself with Spark and the Scala language, we also provide you with two JupyterLab notebooks, namely Notebook 1 and Notebook 2, which you can upload on JupyterLab and run yourself. To get a deeper understanding, and look up the types and definitions of various functions, we recommend that you visit the Spark and Spark SQL documentation."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## a) From SQL to Dataframe (and back again)\n",
    "\n",
    "#### Find for each of the Spark SQL queries an equivalent one that only uses the Dataframe API (or vice versa)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Intitializing Scala interpreter ..."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Spark Web UI available at http://captain01.os.hpc.tuwien.ac.at:9999/proxy/application_1683091297697_2635\n",
       "SparkContext available as 'sc' (version = 3.2.3, master = yarn, app id = application_1683091297697_2635)\n",
       "SparkSession available as 'spark'\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "articlesDF: org.apache.spark.sql.DataFrame = [article_id: string, product_code: string ... 23 more fields]\n",
       "customersDF: org.apache.spark.sql.DataFrame = [customer_id: string, FN: string ... 5 more fields]\n",
       "transactionsDF: org.apache.spark.sql.DataFrame = [t_dat: string, customer_id: string ... 3 more fields]\n"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val articlesDF = spark.read.options(Map(\"header\"->\"true\")).format(\"csv\").load(\"/user/adbs23_shared/hm/articles.csv\")\n",
    "val customersDF = spark.read.options(Map(\"header\"->\"true\")).format(\"csv\").load(\"/user/adbs23_shared/hm/customers.csv\")\n",
    "val transactionsDF = spark.read.options(Map(\"header\"->\"true\")).format(\"csv\").load(\"/user/adbs23_shared/hm/transactions.csv\")\n",
    "\n",
    "\n",
    "// Creating the views for SparkSQL\n",
    "articlesDF.createOrReplaceTempView(\"articles\")\n",
    "customersDF.createOrReplaceTempView(\"customers\")\n",
    "transactionsDF.createOrReplaceTempView(\"transactions\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- article_id: string (nullable = true)\n",
      " |-- product_code: string (nullable = true)\n",
      " |-- prod_name: string (nullable = true)\n",
      " |-- product_type_no: string (nullable = true)\n",
      " |-- product_type_name: string (nullable = true)\n",
      " |-- product_group_name: string (nullable = true)\n",
      " |-- graphical_appearance_no: string (nullable = true)\n",
      " |-- graphical_appearance_name: string (nullable = true)\n",
      " |-- colour_group_code: string (nullable = true)\n",
      " |-- colour_group_name: string (nullable = true)\n",
      " |-- perceived_colour_value_id: string (nullable = true)\n",
      " |-- perceived_colour_value_name: string (nullable = true)\n",
      " |-- perceived_colour_master_id: string (nullable = true)\n",
      " |-- perceived_colour_master_name: string (nullable = true)\n",
      " |-- department_no: string (nullable = true)\n",
      " |-- department_name: string (nullable = true)\n",
      " |-- index_code: string (nullable = true)\n",
      " |-- index_name: string (nullable = true)\n",
      " |-- index_group_no: string (nullable = true)\n",
      " |-- index_group_name: string (nullable = true)\n",
      " |-- section_no: string (nullable = true)\n",
      " |-- section_name: string (nullable = true)\n",
      " |-- garment_group_no: string (nullable = true)\n",
      " |-- garment_group_name: string (nullable = true)\n",
      " |-- detail_desc: string (nullable = true)\n",
      "\n",
      "root\n",
      " |-- customer_id: string (nullable = true)\n",
      " |-- FN: string (nullable = true)\n",
      " |-- Active: string (nullable = true)\n",
      " |-- club_member_status: string (nullable = true)\n",
      " |-- fashion_news_frequency: string (nullable = true)\n",
      " |-- age: string (nullable = true)\n",
      " |-- postal_code: string (nullable = true)\n",
      "\n",
      "root\n",
      " |-- t_dat: string (nullable = true)\n",
      " |-- customer_id: string (nullable = true)\n",
      " |-- article_id: string (nullable = true)\n",
      " |-- price: string (nullable = true)\n",
      " |-- sales_channel_id: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "articlesDF.printSchema()\n",
    "customersDF.printSchema()\n",
    "transactionsDF.printSchema()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Query 1: Transform the given Spark SQL query into the Dataframe API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+\n",
      "|count(1)|\n",
      "+--------+\n",
      "|     138|\n",
      "+--------+\n",
      "\n",
      "== Physical Plan ==\n",
      "AdaptiveSparkPlan isFinalPlan=false\n",
      "+- HashAggregate(keys=[], functions=[count(1)])\n",
      "   +- Exchange SinglePartition, ENSURE_REQUIREMENTS, [plan_id=125]\n",
      "      +- HashAggregate(keys=[], functions=[partial_count(1)])\n",
      "         +- Project\n",
      "            +- Filter (((isnotnull(department_name#31) AND isnotnull(section_name#37)) AND (department_name#31 = Limited Edition)) AND (section_name#37 = Special Collections))\n",
      "               +- FileScan csv [department_name#31,section_name#37] Batched: false, DataFilters: [isnotnull(department_name#31), isnotnull(section_name#37), (department_name#31 = Limited Edition..., Format: CSV, Location: InMemoryFileIndex(1 paths)[hdfs://captain01.os.hpc.tuwien.ac.at:9000/user/adbs23_shared/hm/articl..., PartitionFilters: [], PushedFilters: [IsNotNull(department_name), IsNotNull(section_name), EqualTo(department_name,Limited Edition), E..., ReadSchema: struct<department_name:string,section_name:string>\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "query1: org.apache.spark.sql.DataFrame = [count(1): bigint]\n"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val query1 = spark.sql(\"SELECT COUNT(*) FROM articles WHERE department_name = 'Limited Edition' AND section_name = 'Special Collections' \")\n",
    "query1.show() // 'false' turns of truncation of row entries\n",
    "query1.explain()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+\n",
      "|count(1)|\n",
      "+--------+\n",
      "|     138|\n",
      "+--------+\n",
      "\n",
      "== Physical Plan ==\n",
      "AdaptiveSparkPlan isFinalPlan=false\n",
      "+- HashAggregate(keys=[], functions=[count(1)])\n",
      "   +- Exchange SinglePartition, ENSURE_REQUIREMENTS, [plan_id=249]\n",
      "      +- HashAggregate(keys=[], functions=[partial_count(1)])\n",
      "         +- Project\n",
      "            +- Filter (((isnotnull(department_name#31) AND isnotnull(section_name#37)) AND (department_name#31 = Limited Edition)) AND (section_name#37 = Special Collections))\n",
      "               +- FileScan csv [department_name#31,section_name#37] Batched: false, DataFilters: [isnotnull(department_name#31), isnotnull(section_name#37), (department_name#31 = Limited Edition..., Format: CSV, Location: InMemoryFileIndex(1 paths)[hdfs://captain01.os.hpc.tuwien.ac.at:9000/user/adbs23_shared/hm/articl..., PartitionFilters: [], PushedFilters: [IsNotNull(department_name), IsNotNull(section_name), EqualTo(department_name,Limited Edition), E..., ReadSchema: struct<department_name:string,section_name:string>\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "query1DF: org.apache.spark.sql.Dataset[Long] = [count(1): bigint]\n"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val query1DF = articlesDF.where($\"department_name\" === \"Limited Edition\" && $\"section_name\" === \"Special Collections\").select(count(\"*\"))\n",
    "\n",
    "// ... your solution goes here ...\n",
    "query1DF.show()\n",
    "query1DF.explain()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Query 2: Transform the given Dataframe API query into Spark SQL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----------------------------------------------------------------+-----------------------+\n",
      "|age|postal_code                                                     |count(unresolvedstar())|\n",
      "+---+----------------------------------------------------------------+-----------------------+\n",
      "|18 |7dd2332aac02b1b4aa1b5754dd3ff302e398049482a1837a833925857f22f9f9|0                      |\n",
      "|29 |3d503ddbe4714788f214cd9e7cfe6d26ffdf95f907ff79d2efe3b0cbb2a1c802|1                      |\n",
      "|48 |6bf64e9b563ace70c7f185a9483d001d6b78d94f462de95d634ee10a7a0d0044|1                      |\n",
      "|25 |ac8f1b7c0dce04c73ee65b906a257504ca2d3eccb04cc7e21119c44f94cce080|0                      |\n",
      "|22 |7f400ca33459e114e8eaa175e625bc602a9e588f7f0876e1b1004d4c4cb8eb92|1                      |\n",
      "|20 |0f8ff152a3a406106876d111d067367240a2de164ae5a857ec02a55923efd6ed|0                      |\n",
      "|49 |2b91daa9471d959f5ea58d4d3a50bf116b6033ea14f42d0c26ea0288778ef54a|1                      |\n",
      "|45 |7c72296077cff43341ddfbeb0dbfd9d0c85d9ee0c7fece608a515d16f89a1e6a|0                      |\n",
      "|20 |1e6d0da825ae977fe32183b26c1bc60847b540f4f902931ecc123f8da98e42b0|1                      |\n",
      "|52 |fbadc3783ed8ff1349a52005a1d56a935ebfa466e80459f850e597c01b0bd839|0                      |\n",
      "|25 |1b0d5b3144ef0b6c648ea109dbd9a2a38d447c39813929684e1db31d45f0bfe6|2                      |\n",
      "|35 |c201418900906214532300cdaeb4bfcf7e34aaebc8cda273831a497207f4612a|0                      |\n",
      "|26 |ec044f3f31b1239ccd404d76a47587563193974ad11907b3030657491109b588|0                      |\n",
      "|22 |75da6d06e5ffc94d8e8b966bb5537ce58f9015eb688c367715411892890bbbbe|2                      |\n",
      "|23 |cb9498200024c5e70ad71ec510978760f07a56b9561cbae1a51d58894e4e188d|0                      |\n",
      "|20 |82e52938d5a2ededc41d7eb8dcbe1a623c74e42b6eb5b77730d087d5d9626920|1                      |\n",
      "|30 |d651b17c129ea620912166d8797541a46674d875d1a925c2e36a7de736bb1de9|1                      |\n",
      "|44 |7f1e7d0e1cf448645f5283261dd18c97c872daaecbf8084b1f3b19dc8a31f993|1                      |\n",
      "|30 |3a26503660ed2d0ea1a18b6f5fad26d4a445f6cdde7a1237da6585580c21ea00|0                      |\n",
      "|36 |4ffb8e19db9078236ff8b490c7487b8766fe1bd6736cef65ca9e3aefd557160a|0                      |\n",
      "+---+----------------------------------------------------------------+-----------------------+\n",
      "only showing top 20 rows\n",
      "\n",
      "== Physical Plan ==\n",
      "AdaptiveSparkPlan isFinalPlan=false\n",
      "+- HashAggregate(keys=[age#87, postal_code#88], functions=[count(distinct customer_id#82, FN#83, Active#84, club_member_status#85, fashion_news_frequency#86, age#87, postal_code#88)])\n",
      "   +- Exchange hashpartitioning(age#87, postal_code#88, 200), ENSURE_REQUIREMENTS, [plan_id=242]\n",
      "      +- HashAggregate(keys=[age#87, postal_code#88], functions=[partial_count(distinct customer_id#82, FN#83, Active#84, club_member_status#85, fashion_news_frequency#86, age#87, postal_code#88)])\n",
      "         +- HashAggregate(keys=[age#87, postal_code#88, customer_id#82, FN#83, Active#84, club_member_status#85, fashion_news_frequency#86, age#87, postal_code#88], functions=[])\n",
      "            +- Exchange hashpartitioning(age#87, postal_code#88, customer_id#82, FN#83, Active#84, club_member_status#85, fashion_news_frequency#86, age#87, postal_code#88, 200), ENSURE_REQUIREMENTS, [plan_id=238]\n",
      "               +- HashAggregate(keys=[age#87, postal_code#88, customer_id#82, FN#83, Active#84, club_member_status#85, fashion_news_frequency#86, age#87, postal_code#88], functions=[])\n",
      "                  +- FileScan csv [customer_id#82,FN#83,Active#84,club_member_status#85,fashion_news_frequency#86,age#87,postal_code#88] Batched: false, DataFilters: [], Format: CSV, Location: InMemoryFileIndex(1 paths)[hdfs://captain01.os.hpc.tuwien.ac.at:9000/user/adbs23_shared/hm/custom..., PartitionFilters: [], PushedFilters: [], ReadSchema: struct<customer_id:string,FN:string,Active:string,club_member_status:string,fashion_news_frequenc...\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "query2: org.apache.spark.sql.DataFrame = [age: string, postal_code: string ... 1 more field]\n"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val query2 = customersDF.groupBy(\"age\", \"postal_code\").agg(countDistinct(\"*\"))\n",
    "query2.show(false) // 'false' turns of truncation of row entries\n",
    "query2.explain()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----------------------------------------------------------------+-----+\n",
      "|age|postal_code                                                     |count|\n",
      "+---+----------------------------------------------------------------+-----+\n",
      "|24 |9bad9727c1e78acd230510e14a12510d7a56c5cd6d0a9c8c88496626985b329a|1    |\n",
      "|55 |7d658f624b9e01f8161d4a977d0dc854c0a0e0665eabd8f0ebd9588f83475ac3|0    |\n",
      "|40 |9689774f511926fd680f14994c18a5f1b89f4faa852f4ef557c96acfd024a896|0    |\n",
      "|25 |abc5a4fab86ec7d69f7f9aaa05be3b08cd26a8656363fc0401af1eca0f73a240|0    |\n",
      "|20 |423c8f32db84cdc328d12be02c71002800868f8fd7b16e67d2e3a4656e018646|0    |\n",
      "|22 |4e4708014a3763783b6f932e00b0e8fe72470a407390c2ca8e194773081e55c7|1    |\n",
      "|29 |a9c6e6510c9d0f46d119eecf1301fed74d2f455fa1e251639edcc52284c4c11b|1    |\n",
      "|36 |1f6def8101e8458d17021f7b2a06e5c1b2cf2b920afe28f42453769e7a8ce3f1|1    |\n",
      "|23 |f3aeed11914f14cc4903d0d666c9421c058260d331c202dfde088f32b4e42a69|1    |\n",
      "|24 |6d708d51997a821b7178097b45b9b159822dfe7e8cb23d790d6c58288acabe2a|0    |\n",
      "|30 |e1b879efb06530684cd2d6a5e3d00104ddb23d42204b64d0b9cda6632d2cdba9|1    |\n",
      "|26 |bc2db731376e04f212f0c1e3b98d388442f980bf08a6145eb6c44bc1d46723d4|1    |\n",
      "|26 |4bee5305ba1361b2f458c31eb8d3e562064984c27f1b71befa66012dbeef7d59|0    |\n",
      "|26 |5841521e1dbd87c22c8192de2b5879ff90a5940e304c9ebc67a004caeec08906|1    |\n",
      "|30 |50d2c17aadd20b76a85e789ea3c10b2cf9e1603f1edc752b9533011545136d5a|1    |\n",
      "|51 |721be350866ad966ba94f1c77fae6cd0fa0ba1e98c8067fe1bd7e2b7017ae696|1    |\n",
      "|24 |e34233d97bf3203d4de4f9d20a940de1e1c1fe701b0a8facefd9d012d2f847b5|1    |\n",
      "|21 |ec6d64ccd255c2941fa139dd728ce0f4e40e9972029ec441b0578be345ab8744|0    |\n",
      "|55 |df6f603e9792d8d2e601c76228ecaa6a427148c0367228abc695255408017bf0|0    |\n",
      "|27 |0f93b2989367eaa180e476ae385ea77badede6fd4fda61f2d0051657a71e8863|0    |\n",
      "+---+----------------------------------------------------------------+-----+\n",
      "only showing top 20 rows\n",
      "\n",
      "== Physical Plan ==\n",
      "AdaptiveSparkPlan isFinalPlan=false\n",
      "+- HashAggregate(keys=[age#87, postal_code#88], functions=[count(distinct customer_id#82, FN#83, Active#84, club_member_status#85, fashion_news_frequency#86, age#87, postal_code#88)])\n",
      "   +- Exchange hashpartitioning(age#87, postal_code#88, 200), ENSURE_REQUIREMENTS, [plan_id=600]\n",
      "      +- HashAggregate(keys=[age#87, postal_code#88], functions=[partial_count(distinct customer_id#82, FN#83, Active#84, club_member_status#85, fashion_news_frequency#86, age#87, postal_code#88)])\n",
      "         +- HashAggregate(keys=[age#87, postal_code#88, customer_id#82, FN#83, Active#84, club_member_status#85, fashion_news_frequency#86, age#87, postal_code#88], functions=[])\n",
      "            +- Exchange hashpartitioning(age#87, postal_code#88, customer_id#82, FN#83, Active#84, club_member_status#85, fashion_news_frequency#86, age#87, postal_code#88, 200), ENSURE_REQUIREMENTS, [plan_id=596]\n",
      "               +- HashAggregate(keys=[age#87, postal_code#88, customer_id#82, FN#83, Active#84, club_member_status#85, fashion_news_frequency#86, age#87, postal_code#88], functions=[])\n",
      "                  +- FileScan csv [customer_id#82,FN#83,Active#84,club_member_status#85,fashion_news_frequency#86,age#87,postal_code#88] Batched: false, DataFilters: [], Format: CSV, Location: InMemoryFileIndex(1 paths)[hdfs://captain01.os.hpc.tuwien.ac.at:9000/user/adbs23_shared/hm/custom..., PartitionFilters: [], PushedFilters: [], ReadSchema: struct<customer_id:string,FN:string,Active:string,club_member_status:string,fashion_news_frequenc...\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "query2DF: org.apache.spark.sql.DataFrame = [age: string, postal_code: string ... 1 more field]\n"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val query2DF = spark.sql(\"SELECT age, postal_code, COUNT(DISTINCT *) as count FROM customers GROUP BY age, postal_code;\")\n",
    "query2DF.show(false) // 'false' turns of truncation of row entries\n",
    "query2DF.explain()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Query 3: Transform the given Dataframe API query into Spark SQL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------+--------------------------+\n",
      "|garment_group_no|prod_name                 |\n",
      "+----------------+--------------------------+\n",
      "|1001            |SULIMA jkt                |\n",
      "|1001            |NORA shorts               |\n",
      "|1001            |WC T-shirts               |\n",
      "|1001            |EQ TERRIER TEE            |\n",
      "|1001            |DIV Zebra sweater         |\n",
      "|1001            |W DAVID TRS EQ            |\n",
      "|1001            |BB 2-pack Carter shorts   |\n",
      "|1001            |SULIMA  jkt               |\n",
      "|1001            |DIV Maja cardigan         |\n",
      "|1001            |RC MARTINI TEE            |\n",
      "|1001            |EQ TIKKA TEE              |\n",
      "|1001            |NORA RW shorts innerbriefs|\n",
      "|1001            |Supreme tights            |\n",
      "|1001            |BB Bolt T-shirt           |\n",
      "|1001            |SUPREME Fancy Tights      |\n",
      "|1001            |BB 2-pack T-shirts        |\n",
      "|1001            |KATE TEE                  |\n",
      "|1001            |WC Shorts                 |\n",
      "|1001            |W GENOVA TRS EQ           |\n",
      "|1001            |SUPREME BASIC tights      |\n",
      "+----------------+--------------------------+\n",
      "only showing top 20 rows\n",
      "\n",
      "== Physical Plan ==\n",
      "AdaptiveSparkPlan isFinalPlan=false\n",
      "+- Project [garment_group_no#38, prod_name#18]\n",
      "   +- Filter (seqnum#175 > 1)\n",
      "      +- Window [row_number() windowspecdefinition(garment_group_no#38, _w0#178L DESC NULLS LAST, specifiedwindowframe(RowFrame, unboundedpreceding$(), currentrow$())) AS seqnum#175], [garment_group_no#38], [_w0#178L DESC NULLS LAST]\n",
      "         +- Sort [garment_group_no#38 ASC NULLS FIRST, _w0#178L DESC NULLS LAST], false, 0\n",
      "            +- Exchange hashpartitioning(garment_group_no#38, 200), ENSURE_REQUIREMENTS, [plan_id=406]\n",
      "               +- HashAggregate(keys=[garment_group_no#38, prod_name#18], functions=[count(prod_name#18)])\n",
      "                  +- Exchange hashpartitioning(garment_group_no#38, prod_name#18, 200), ENSURE_REQUIREMENTS, [plan_id=403]\n",
      "                     +- HashAggregate(keys=[garment_group_no#38, prod_name#18], functions=[partial_count(prod_name#18)])\n",
      "                        +- FileScan csv [prod_name#18,garment_group_no#38] Batched: false, DataFilters: [], Format: CSV, Location: InMemoryFileIndex(1 paths)[hdfs://captain01.os.hpc.tuwien.ac.at:9000/user/adbs23_shared/hm/articl..., PartitionFilters: [], PushedFilters: [], ReadSchema: struct<prod_name:string,garment_group_no:string>\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "subquery: String = ( select garment_group_no , prod_name, row_number() over (partition by garment_group_no order by count(prod_name) desc) as seqnum from articles a1 group by garment_group_no, prod_name )\n",
       "query: String = \"SELECT garment_group_no, prod_name FROM ( select garment_group_no , prod_name, row_number() over (partition by garment_group_no order by count(prod_name) desc) as seqnum from articles a1 group by garment_group_no, prod_name ) WHERE seqnum > 1 \"\n",
       "query3: org.apache.spark.sql.DataFrame = [garment_group_no: string, prod_name: string]\n"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// row_number() over partition by garment_group_no order by count(prod_name) desc orders by desc garment_group_no and then calculates sequential number for each row\n",
    "\n",
    "val subquery = \n",
    "        \"( select garment_group_no , prod_name, \" +\n",
    "        \"row_number() over (partition by garment_group_no order by count(prod_name) desc) as seqnum \" +\n",
    "        \"from articles a1 \" +\n",
    "        \"group by garment_group_no, prod_name )\"\n",
    "\n",
    "// You can find more details on the row_number function here: https://sparkbyexamples.com/spark/spark-sql-add-row-number-dataframe/\n",
    "// To define windows for row number, you will need to import the Window object via \"import org.apache.spark.sql.expressions.Window\"\n",
    "\n",
    "val query = \"SELECT garment_group_no, prod_name \" +\n",
    "            \"FROM \" + subquery +\n",
    "            \" WHERE seqnum > 1 \"\n",
    "\n",
    "\n",
    "val query3 = spark.sql(query)\n",
    "query3.show(false) // 'false' turns of truncation of row entries\n",
    "query3.explain()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------+--------------------------+\n",
      "|garment_group_no|prod_name                 |\n",
      "+----------------+--------------------------+\n",
      "|1001            |SULIMA jkt                |\n",
      "|1001            |NORA shorts               |\n",
      "|1001            |WC T-shirts               |\n",
      "|1001            |EQ TERRIER TEE            |\n",
      "|1001            |DIV Zebra sweater         |\n",
      "|1001            |W DAVID TRS EQ            |\n",
      "|1001            |BB 2-pack Carter shorts   |\n",
      "|1001            |SULIMA  jkt               |\n",
      "|1001            |DIV Maja cardigan         |\n",
      "|1001            |RC MARTINI TEE            |\n",
      "|1001            |EQ TIKKA TEE              |\n",
      "|1001            |NORA RW shorts innerbriefs|\n",
      "|1001            |Supreme tights            |\n",
      "|1001            |BB Bolt T-shirt           |\n",
      "|1001            |SUPREME Fancy Tights      |\n",
      "|1001            |BB 2-pack T-shirts        |\n",
      "|1001            |KATE TEE                  |\n",
      "|1001            |WC Shorts                 |\n",
      "|1001            |W GENOVA TRS EQ           |\n",
      "|1001            |SUPREME BASIC tights      |\n",
      "+----------------+--------------------------+\n",
      "only showing top 20 rows\n",
      "\n",
      "== Physical Plan ==\n",
      "AdaptiveSparkPlan isFinalPlan=false\n",
      "+- Project [garment_group_no#38, prod_name#18]\n",
      "   +- Filter (seqnum#383 > 1)\n",
      "      +- Window [row_number() windowspecdefinition(garment_group_no#38, prod_count#378L DESC NULLS LAST, specifiedwindowframe(RowFrame, unboundedpreceding$(), currentrow$())) AS seqnum#383], [garment_group_no#38], [prod_count#378L DESC NULLS LAST]\n",
      "         +- Sort [garment_group_no#38 ASC NULLS FIRST, prod_count#378L DESC NULLS LAST], false, 0\n",
      "            +- Exchange hashpartitioning(garment_group_no#38, 200), ENSURE_REQUIREMENTS, [plan_id=931]\n",
      "               +- HashAggregate(keys=[garment_group_no#38, prod_name#18], functions=[count(1)])\n",
      "                  +- Exchange hashpartitioning(garment_group_no#38, prod_name#18, 200), ENSURE_REQUIREMENTS, [plan_id=928]\n",
      "                     +- HashAggregate(keys=[garment_group_no#38, prod_name#18], functions=[partial_count(1)])\n",
      "                        +- FileScan csv [prod_name#18,garment_group_no#38] Batched: false, DataFilters: [], Format: CSV, Location: InMemoryFileIndex(1 paths)[hdfs://captain01.os.hpc.tuwien.ac.at:9000/user/adbs23_shared/hm/articl..., PartitionFilters: [], PushedFilters: [], ReadSchema: struct<prod_name:string,garment_group_no:string>\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "import org.apache.spark.sql.expressions.Window\n",
       "import org.apache.spark.sql.functions.{col, count, row_number}\n",
       "windowSpec: org.apache.spark.sql.expressions.WindowSpec = org.apache.spark.sql.expressions.WindowSpec@151de53b\n",
       "subquery: org.apache.spark.sql.DataFrame = [garment_group_no: string, prod_name: string ... 2 more fields]\n",
       "query3: org.apache.spark.sql.DataFrame = [garment_group_no: string, prod_name: string]\n"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.spark.sql.expressions.Window\n",
    "import org.apache.spark.sql.functions.{col, count, row_number}\n",
    "\n",
    "val windowSpec = Window.partitionBy(\"garment_group_no\").orderBy(col(\"prod_count\").desc)\n",
    "val subquery = articlesDF\n",
    "  .groupBy(\"garment_group_no\", \"prod_name\")\n",
    "  .agg(count(\"*\").alias(\"prod_count\"))\n",
    "  .withColumn(\"seqnum\", row_number().over(windowSpec))\n",
    "\n",
    "val query3 = subquery.filter(col(\"seqnum\") > 1)\n",
    "  .select(\"garment_group_no\", \"prod_name\")\n",
    "\n",
    "query3.show(false) // 'false' turns of truncation of row entries\n",
    "query3.explain()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Query 4: Transform the given Dataframe API query into Spark SQL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+--------------+\n",
      "|article_id|count_same_dep|\n",
      "+----------+--------------+\n",
      "|0630313008|1359          |\n",
      "|0735131001|1359          |\n",
      "|0554598018|1359          |\n",
      "|0778064012|1359          |\n",
      "|0525335018|1359          |\n",
      "|0760834001|1359          |\n",
      "|0753061006|1359          |\n",
      "|0856667005|1359          |\n",
      "|0610776072|1359          |\n",
      "|0740824002|1359          |\n",
      "|0717063001|1359          |\n",
      "|0819520001|1359          |\n",
      "|0904584008|1359          |\n",
      "|0456163009|1359          |\n",
      "|0610776106|1359          |\n",
      "|0820484001|1359          |\n",
      "|0880001002|1359          |\n",
      "|0506110001|1359          |\n",
      "|0565379022|1359          |\n",
      "|0786304002|1359          |\n",
      "+----------+--------------+\n",
      "only showing top 20 rows\n",
      "\n",
      "== Physical Plan ==\n",
      "AdaptiveSparkPlan isFinalPlan=false\n",
      "+- Sort [count_same_dep#410L DESC NULLS LAST], true, 0\n",
      "   +- Exchange rangepartitioning(count_same_dep#410L DESC NULLS LAST, 200), ENSURE_REQUIREMENTS, [plan_id=1115]\n",
      "      +- HashAggregate(keys=[article_id#16], functions=[count(1)])\n",
      "         +- Exchange hashpartitioning(article_id#16, 200), ENSURE_REQUIREMENTS, [plan_id=1112]\n",
      "            +- HashAggregate(keys=[article_id#16], functions=[partial_count(1)])\n",
      "               +- Project [article_id#16]\n",
      "                  +- BroadcastHashJoin [department_no#30], [department_no#316], Inner, BuildRight, false\n",
      "                     :- HashAggregate(keys=[article_id#16, department_no#30], functions=[])\n",
      "                     :  +- Exchange hashpartitioning(article_id#16, department_no#30, 200), ENSURE_REQUIREMENTS, [plan_id=1104]\n",
      "                     :     +- HashAggregate(keys=[article_id#16, department_no#30], functions=[])\n",
      "                     :        +- Project [article_id#16, department_no#30]\n",
      "                     :           +- BroadcastHashJoin [article_id#114], [article_id#16], Inner, BuildRight, false\n",
      "                     :              :- Project [article_id#114]\n",
      "                     :              :  +- SortMergeJoin [customer_id#82], [customer_id#113], Inner\n",
      "                     :              :     :- Sort [customer_id#82 ASC NULLS FIRST], false, 0\n",
      "                     :              :     :  +- Exchange hashpartitioning(customer_id#82, 200), ENSURE_REQUIREMENTS, [plan_id=1092]\n",
      "                     :              :     :     +- Project [customer_id#82]\n",
      "                     :              :     :        +- Filter ((isnotnull(age#87) AND (cast(age#87 as int) > 40)) AND isnotnull(customer_id#82))\n",
      "                     :              :     :           +- FileScan csv [customer_id#82,age#87] Batched: false, DataFilters: [isnotnull(age#87), (cast(age#87 as int) > 40), isnotnull(customer_id#82)], Format: CSV, Location: InMemoryFileIndex(1 paths)[hdfs://captain01.os.hpc.tuwien.ac.at:9000/user/adbs23_shared/hm/custom..., PartitionFilters: [], PushedFilters: [IsNotNull(age), IsNotNull(customer_id)], ReadSchema: struct<customer_id:string,age:string>\n",
      "                     :              :     +- Sort [customer_id#113 ASC NULLS FIRST], false, 0\n",
      "                     :              :        +- Exchange hashpartitioning(customer_id#113, 200), ENSURE_REQUIREMENTS, [plan_id=1093]\n",
      "                     :              :           +- Filter (isnotnull(customer_id#113) AND isnotnull(article_id#114))\n",
      "                     :              :              +- FileScan csv [customer_id#113,article_id#114] Batched: false, DataFilters: [isnotnull(customer_id#113), isnotnull(article_id#114)], Format: CSV, Location: InMemoryFileIndex(1 paths)[hdfs://captain01.os.hpc.tuwien.ac.at:9000/user/adbs23_shared/hm/transa..., PartitionFilters: [], PushedFilters: [IsNotNull(customer_id), IsNotNull(article_id)], ReadSchema: struct<customer_id:string,article_id:string>\n",
      "                     :              +- BroadcastExchange HashedRelationBroadcastMode(List(input[0, string, true]),false), [plan_id=1099]\n",
      "                     :                 +- Project [article_id#16, department_no#30]\n",
      "                     :                    +- Filter ((((department_name#31 = Jersey Basic) OR (department_name#31 = Shirt)) AND isnotnull(article_id#16)) AND isnotnull(department_no#30))\n",
      "                     :                       +- FileScan csv [article_id#16,department_no#30,department_name#31] Batched: false, DataFilters: [((department_name#31 = Jersey Basic) OR (department_name#31 = Shirt)), isnotnull(article_id#16),..., Format: CSV, Location: InMemoryFileIndex(1 paths)[hdfs://captain01.os.hpc.tuwien.ac.at:9000/user/adbs23_shared/hm/articl..., PartitionFilters: [], PushedFilters: [Or(EqualTo(department_name,Jersey Basic),EqualTo(department_name,Shirt)), IsNotNull(article_id),..., ReadSchema: struct<article_id:string,department_no:string,department_name:string>\n",
      "                     +- BroadcastExchange HashedRelationBroadcastMode(List(input[0, string, false]),false), [plan_id=1107]\n",
      "                        +- Filter isnotnull(department_no#316)\n",
      "                           +- FileScan csv [department_no#316] Batched: false, DataFilters: [isnotnull(department_no#316)], Format: CSV, Location: InMemoryFileIndex(1 paths)[hdfs://captain01.os.hpc.tuwien.ac.at:9000/user/adbs23_shared/hm/articl..., PartitionFilters: [], PushedFilters: [IsNotNull(department_no)], ReadSchema: struct<department_no:string>\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "query4: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row] = [article_id: string, count_same_dep: bigint]\n"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val query4 = customersDF\n",
    "      .join(transactionsDF, customersDF(\"customer_id\") === transactionsDF(\"customer_id\"))\n",
    "      .join(articlesDF, articlesDF(\"article_id\") === transactionsDF(\"article_id\"))\n",
    "      .filter(\"age > 40 AND (department_name = 'Jersey Basic' OR department_name = 'Shirt') \")\n",
    "      .select(articlesDF(\"article_id\"), articlesDF(\"department_no\")).distinct\n",
    "      .alias(\"df1\")\n",
    "      .join(articlesDF.alias(\"df2\"), $\"df1.department_no\" === $\"df2.department_no\")\n",
    "      .groupBy(articlesDF.as(\"df1\")(\"article_id\"))\n",
    "      .agg(count(\"*\").as(\"count_same_dep\"))\n",
    "      .orderBy(desc(\"count_same_dep\"))\n",
    "query4.show(false) // 'false' turns of truncation of row entries\n",
    "query4.explain()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+--------------+\n",
      "|article_id|count_same_dep|\n",
      "+----------+--------------+\n",
      "|0735131001|1359          |\n",
      "|0554598018|1359          |\n",
      "|0372008012|1359          |\n",
      "|0778064012|1359          |\n",
      "|0753061006|1359          |\n",
      "|0760834001|1359          |\n",
      "|0610776072|1359          |\n",
      "|0856667005|1359          |\n",
      "|0717063001|1359          |\n",
      "|0740824002|1359          |\n",
      "|0904584008|1359          |\n",
      "|0819520001|1359          |\n",
      "|0610776106|1359          |\n",
      "|0629420020|1359          |\n",
      "|0880001002|1359          |\n",
      "|0630313008|1359          |\n",
      "|0608458002|1359          |\n",
      "|0506110001|1359          |\n",
      "|0565379022|1359          |\n",
      "|0786304002|1359          |\n",
      "+----------+--------------+\n",
      "only showing top 20 rows\n",
      "\n",
      "== Physical Plan ==\n",
      "AdaptiveSparkPlan isFinalPlan=false\n",
      "+- Sort [count_same_dep#403L DESC NULLS LAST], true, 0\n",
      "   +- Exchange rangepartitioning(count_same_dep#403L DESC NULLS LAST, 200), ENSURE_REQUIREMENTS, [plan_id=1649]\n",
      "      +- HashAggregate(keys=[article_id#16], functions=[count(1)])\n",
      "         +- Exchange hashpartitioning(article_id#16, 200), ENSURE_REQUIREMENTS, [plan_id=1646]\n",
      "            +- HashAggregate(keys=[article_id#16], functions=[partial_count(1)])\n",
      "               +- Project [article_id#16]\n",
      "                  +- BroadcastHashJoin [department_no#30], [department_no#418], Inner, BuildRight, false\n",
      "                     :- HashAggregate(keys=[article_id#16, department_no#30], functions=[])\n",
      "                     :  +- Exchange hashpartitioning(article_id#16, department_no#30, 200), ENSURE_REQUIREMENTS, [plan_id=1638]\n",
      "                     :     +- HashAggregate(keys=[article_id#16, department_no#30], functions=[])\n",
      "                     :        +- Project [article_id#16, department_no#30]\n",
      "                     :           +- BroadcastHashJoin [article_id#114], [article_id#16], Inner, BuildRight, false\n",
      "                     :              :- Project [article_id#114]\n",
      "                     :              :  +- SortMergeJoin [customer_id#82], [customer_id#113], Inner\n",
      "                     :              :     :- Sort [customer_id#82 ASC NULLS FIRST], false, 0\n",
      "                     :              :     :  +- Exchange hashpartitioning(customer_id#82, 200), ENSURE_REQUIREMENTS, [plan_id=1626]\n",
      "                     :              :     :     +- Project [customer_id#82]\n",
      "                     :              :     :        +- Filter ((isnotnull(age#87) AND (cast(age#87 as int) > 40)) AND isnotnull(customer_id#82))\n",
      "                     :              :     :           +- FileScan csv [customer_id#82,age#87] Batched: false, DataFilters: [isnotnull(age#87), (cast(age#87 as int) > 40), isnotnull(customer_id#82)], Format: CSV, Location: InMemoryFileIndex(1 paths)[hdfs://captain01.os.hpc.tuwien.ac.at:9000/user/adbs23_shared/hm/custom..., PartitionFilters: [], PushedFilters: [IsNotNull(age), IsNotNull(customer_id)], ReadSchema: struct<customer_id:string,age:string>\n",
      "                     :              :     +- Sort [customer_id#113 ASC NULLS FIRST], false, 0\n",
      "                     :              :        +- Exchange hashpartitioning(customer_id#113, 200), ENSURE_REQUIREMENTS, [plan_id=1627]\n",
      "                     :              :           +- Filter (isnotnull(customer_id#113) AND isnotnull(article_id#114))\n",
      "                     :              :              +- FileScan csv [customer_id#113,article_id#114] Batched: false, DataFilters: [isnotnull(customer_id#113), isnotnull(article_id#114)], Format: CSV, Location: InMemoryFileIndex(1 paths)[hdfs://captain01.os.hpc.tuwien.ac.at:9000/user/adbs23_shared/hm/transa..., PartitionFilters: [], PushedFilters: [IsNotNull(customer_id), IsNotNull(article_id)], ReadSchema: struct<customer_id:string,article_id:string>\n",
      "                     :              +- BroadcastExchange HashedRelationBroadcastMode(List(input[0, string, true]),false), [plan_id=1633]\n",
      "                     :                 +- Project [article_id#16, department_no#30]\n",
      "                     :                    +- Filter ((((department_name#31 = Jersey Basic) OR (department_name#31 = Shirt)) AND isnotnull(article_id#16)) AND isnotnull(department_no#30))\n",
      "                     :                       +- FileScan csv [article_id#16,department_no#30,department_name#31] Batched: false, DataFilters: [((department_name#31 = Jersey Basic) OR (department_name#31 = Shirt)), isnotnull(article_id#16),..., Format: CSV, Location: InMemoryFileIndex(1 paths)[hdfs://captain01.os.hpc.tuwien.ac.at:9000/user/adbs23_shared/hm/articl..., PartitionFilters: [], PushedFilters: [Or(EqualTo(department_name,Jersey Basic),EqualTo(department_name,Shirt)), IsNotNull(article_id),..., ReadSchema: struct<article_id:string,department_no:string,department_name:string>\n",
      "                     +- BroadcastExchange HashedRelationBroadcastMode(List(input[0, string, false]),false), [plan_id=1641]\n",
      "                        +- Filter isnotnull(department_no#418)\n",
      "                           +- FileScan csv [department_no#418] Batched: false, DataFilters: [isnotnull(department_no#418)], Format: CSV, Location: InMemoryFileIndex(1 paths)[hdfs://captain01.os.hpc.tuwien.ac.at:9000/user/adbs23_shared/hm/articl..., PartitionFilters: [], PushedFilters: [IsNotNull(department_no)], ReadSchema: struct<department_no:string>\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "query: String =\n",
       "\"\n",
       "  SELECT df1.article_id, COUNT(*) AS count_same_dep\n",
       "  FROM (\n",
       "    SELECT DISTINCT articles.article_id, articles.department_no\n",
       "    FROM customers\n",
       "    JOIN transactions ON customers.customer_id = transactions.customer_id\n",
       "    JOIN articles ON articles.article_id = transactions.article_id\n",
       "    WHERE customers.age > 40 AND (articles.department_name = 'Jersey Basic' OR articles.department_name = 'Shirt')\n",
       "  ) AS df1\n",
       "  JOIN articles AS df2 ON df1.department_no = df2.department_no\n",
       "  GROUP BY df1.article_id\n",
       "  ORDER BY count_same_dep DESC\n",
       "\"\n",
       "query4: org.apache.spark.sql.DataFrame = [article_id: string, count_same_dep: bigint]\n"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "customersDF.createOrReplaceTempView(\"customers\")\n",
    "transactionsDF.createOrReplaceTempView(\"transactions\")\n",
    "articlesDF.createOrReplaceTempView(\"articles\")\n",
    "\n",
    "val query = \"\"\"\n",
    "  SELECT df1.article_id, COUNT(*) AS count_same_dep\n",
    "  FROM (\n",
    "    SELECT DISTINCT articles.article_id, articles.department_no\n",
    "    FROM customers\n",
    "    JOIN transactions ON customers.customer_id = transactions.customer_id\n",
    "    JOIN articles ON articles.article_id = transactions.article_id\n",
    "    WHERE customers.age > 40 AND (articles.department_name = 'Jersey Basic' OR articles.department_name = 'Shirt')\n",
    "  ) AS df1\n",
    "  JOIN articles AS df2 ON df1.department_no = df2.department_no\n",
    "  GROUP BY df1.article_id\n",
    "  ORDER BY count_same_dep DESC\n",
    "\"\"\"\n",
    "\n",
    "val query4 = spark.sql(query)\n",
    "query4.show(false) // 'false' turns off truncation of row entries\n",
    "query4.explain()\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "- ### **b) Wide and Narrow Dependencies**\n",
    "\n",
    "You are given a DAG visualisation of how Spark executed a query in various stages, with arrows indicating various parts of the query plan. \n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is the query plan of query4. "
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "== Physical Plan ==\n",
    "AdaptiveSparkPlan isFinalPlan=false\n",
    "+- Sort [count_same_dep#958L DESC NULLS LAST], true, 0\n",
    "   +- Exchange rangepartitioning(count_same_dep#958L DESC NULLS LAST, 200), ENSURE_REQUIREMENTS, [plan_id=3451]\n",
    "      +- HashAggregate(keys=[article_id#16], functions=[count(1)])\n",
    "         +- Exchange hashpartitioning(article_id#16, 200), ENSURE_REQUIREMENTS, [plan_id=3448]\n",
    "            +- HashAggregate(keys=[article_id#16], functions=[partial_count(1)])\n",
    "               +- Project [article_id#16]\n",
    "                  +- BroadcastHashJoin [department_no#30], [department_no#864], Inner, BuildRight, false\n",
    "                     :- HashAggregate(keys=[article_id#16, department_no#30], functions=[])\n",
    "                     :  +- Exchange hashpartitioning(article_id#16, department_no#30, 200), ENSURE_REQUIREMENTS, [plan_id=3440]\n",
    "                     :     +- HashAggregate(keys=[article_id#16, department_no#30], functions=[])\n",
    "                     :        +- Project [article_id#16, department_no#30]\n",
    "                     :           +- BroadcastHashJoin [article_id#114], [article_id#16], Inner, BuildRight, false\n",
    "                     :              :- Project [article_id#114]\n",
    "                     :              :  +- SortMergeJoin [customer_id#82], [customer_id#113], Inner\n",
    "                     :              :     :- Sort [customer_id#82 ASC NULLS FIRST], false, 0\n",
    "                     :              :     :  +- Exchange hashpartitioning(customer_id#82, 200), ENSURE_REQUIREMENTS, [plan_id=3428]\n",
    "                     :              :     :     +- Project [customer_id#82]\n",
    "                     :              :     :        +- Filter ((isnotnull(age#87) AND (cast(age#87 as int) > 40)) AND isnotnull(customer_id#82))\n",
    "                     :              :     :           +- FileScan csv [customer_id#82,age#87] Batched: false, DataFilters: [isnotnull(age#87), (cast(age#87 as int) > 40), isnotnull(customer_id#82)], Format: CSV, Location: InMemoryFileIndex(1 paths)[hdfs://captain01.os.hpc.tuwien.ac.at:9000/user/adbs23_shared/hm/custom..., PartitionFilters: [], PushedFilters: [IsNotNull(age), IsNotNull(customer_id)], ReadSchema: struct<customer_id:string,age:string>\n",
    "                     :              :     +- Sort [customer_id#113 ASC NULLS FIRST], false, 0\n",
    "                     :              :        +- Exchange hashpartitioning(customer_id#113, 200), ENSURE_REQUIREMENTS, [plan_id=3429]\n",
    "                     :              :           +- Filter (isnotnull(customer_id#113) AND isnotnull(article_id#114))\n",
    "                     :              :              +- FileScan csv [customer_id#113,article_id#114] Batched: false, DataFilters: [isnotnull(customer_id#113), isnotnull(article_id#114)], Format: CSV, Location: InMemoryFileIndex(1 paths)[hdfs://captain01.os.hpc.tuwien.ac.at:9000/user/adbs23_shared/hm/transa..., PartitionFilters: [], PushedFilters: [IsNotNull(customer_id), IsNotNull(article_id)], ReadSchema: struct<customer_id:string,article_id:string>\n",
    "                     :              +- BroadcastExchange HashedRelationBroadcastMode(List(input[0, string, true]),false), [plan_id=3435]\n",
    "                     :                 +- Project [article_id#16, department_no#30]\n",
    "                     :                    +- Filter ((((department_name#31 = Jersey Basic) OR (department_name#31 = Shirt)) AND isnotnull(article_id#16)) AND isnotnull(department_no#30))\n",
    "                     :                       +- FileScan csv [article_id#16,department_no#30,department_name#31] Batched: false, DataFilters: [((department_name#31 = Jersey Basic) OR (department_name#31 = Shirt)), isnotnull(article_id#16),..., Format: CSV, Location: InMemoryFileIndex(1 paths)[hdfs://captain01.os.hpc.tuwien.ac.at:9000/user/adbs23_shared/hm/articl..., PartitionFilters: [], PushedFilters: [Or(EqualTo(department_name,Jersey Basic),EqualTo(department_name,Shirt)), IsNotNull(article_id),..., ReadSchema: struct<article_id:string,department_no:string,department_name:string>\n",
    "                     +- BroadcastExchange HashedRelationBroadcastMode(List(input[0, string, false]),false), [plan_id=3443]\n",
    "                        +- Filter isnotnull(department_no#864)\n",
    "                           +- FileScan csv [department_no#864] Batched: false, DataFilters: [isnotnull(department_no#864)], Format: CSV, Location: InMemoryFileIndex(1 paths)[hdfs://captain01.os.hpc.tuwien.ac.at:9000/user/adbs23_shared/hm/articl..., PartitionFilters: [], PushedFilters: [IsNotNull(department_no)], ReadSchema: struct<department_no:string>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is the final query plan which Spark produces (exported from the internal Web UI)."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"adbs1.png\" width=\"1200\"  align=\"left\" />\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Analyse the dependencies and stages of the queries, and try to determine which commands of the query plan are  executed as wide dependencies and which as narrow dependencies.\n",
    "\n",
    "# Solution\n",
    "\n",
    "## Example Operations above with narrow dependencies: \n",
    "\n",
    "* Any Project operation\n",
    "* Any Filter operation\n",
    "\n",
    "## Operations above with wide dependencies\n",
    "\n",
    "* sortMergeJoin -- since the contents of parking and medallion tables need to be brought a single node\n",
    "* sorting -- presumably since neither tables was previously in a single location. This sorting happens in a separate node from the one where the sortMergejoin is performed. Perhaps to reduce communication costs. \n",
    "\n",
    "The \"Exchange\" operation is always a wide dependency, in the sense of linking data from one stage to the next. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "spylon-kernel",
   "language": "scala",
   "name": "spylon-kernel"
  },
  "language_info": {
   "codemirror_mode": "text/x-scala",
   "file_extension": ".scala",
   "help_links": [
    {
     "text": "MetaKernel Magics",
     "url": "https://metakernel.readthedocs.io/en/latest/source/README.html"
    }
   ],
   "mimetype": "text/x-scala",
   "name": "scala",
   "pygments_lexer": "scala",
   "version": "0.4.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
