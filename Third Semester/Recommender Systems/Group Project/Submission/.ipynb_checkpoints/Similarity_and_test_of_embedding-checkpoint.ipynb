{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3f0e3799-1472-4ccb-8e3d-0313621412c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting langchain\n",
      "  Downloading langchain-0.0.229-py3-none-any.whl (1.3 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m17.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: async-timeout<5.0.0,>=4.0.0 in /opt/conda/lib/python3.10/site-packages (from langchain) (4.0.2)\n",
      "Collecting openapi-schema-pydantic<2.0,>=1.2\n",
      "  Using cached openapi_schema_pydantic-1.2.4-py3-none-any.whl (90 kB)\n",
      "Collecting numexpr<3.0.0,>=2.8.4\n",
      "  Using cached numexpr-2.8.4-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (381 kB)\n",
      "Collecting pydantic<2,>=1\n",
      "  Using cached pydantic-1.10.11-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.1 MB)\n",
      "Requirement already satisfied: PyYAML>=5.4.1 in /opt/conda/lib/python3.10/site-packages (from langchain) (6.0)\n",
      "Collecting langchainplus-sdk<0.0.21,>=0.0.20\n",
      "  Using cached langchainplus_sdk-0.0.20-py3-none-any.whl (25 kB)\n",
      "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /opt/conda/lib/python3.10/site-packages (from langchain) (3.8.4)\n",
      "Requirement already satisfied: requests<3,>=2 in /opt/conda/lib/python3.10/site-packages (from langchain) (2.28.2)\n",
      "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /opt/conda/lib/python3.10/site-packages (from langchain) (2.0.1)\n",
      "Requirement already satisfied: tenacity<9.0.0,>=8.1.0 in /opt/conda/lib/python3.10/site-packages (from langchain) (8.2.2)\n",
      "Collecting dataclasses-json<0.6.0,>=0.5.7\n",
      "  Using cached dataclasses_json-0.5.9-py3-none-any.whl (26 kB)\n",
      "Requirement already satisfied: numpy<2,>=1 in /opt/conda/lib/python3.10/site-packages (from langchain) (1.23.5)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /opt/conda/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (6.0.4)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /opt/conda/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.3.1)\n",
      "Requirement already satisfied: charset-normalizer<4.0,>=2.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (2.1.1)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.8.2)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (22.2.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /opt/conda/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.3.3)\n",
      "Collecting typing-inspect>=0.4.0\n",
      "  Using cached typing_inspect-0.9.0-py3-none-any.whl (8.8 kB)\n",
      "Collecting marshmallow<4.0.0,>=3.3.0\n",
      "  Using cached marshmallow-3.19.0-py3-none-any.whl (49 kB)\n",
      "Collecting marshmallow-enum<2.0.0,>=1.5.1\n",
      "  Using cached marshmallow_enum-1.5.1-py2.py3-none-any.whl (4.2 kB)\n",
      "Requirement already satisfied: typing-extensions>=4.2.0 in /opt/conda/lib/python3.10/site-packages (from pydantic<2,>=1->langchain) (4.4.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests<3,>=2->langchain) (2022.12.7)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests<3,>=2->langchain) (3.4)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests<3,>=2->langchain) (1.26.14)\n",
      "Requirement already satisfied: greenlet!=0.4.17 in /opt/conda/lib/python3.10/site-packages (from SQLAlchemy<3,>=1.4->langchain) (2.0.2)\n",
      "Requirement already satisfied: packaging>=17.0 in /opt/conda/lib/python3.10/site-packages (from marshmallow<4.0.0,>=3.3.0->dataclasses-json<0.6.0,>=0.5.7->langchain) (23.0)\n",
      "Requirement already satisfied: mypy-extensions>=0.3.0 in /opt/conda/lib/python3.10/site-packages (from typing-inspect>=0.4.0->dataclasses-json<0.6.0,>=0.5.7->langchain) (1.0.0)\n",
      "Installing collected packages: typing-inspect, pydantic, numexpr, marshmallow, openapi-schema-pydantic, marshmallow-enum, langchainplus-sdk, dataclasses-json, langchain\n",
      "  Attempting uninstall: numexpr\n",
      "    Found existing installation: numexpr 2.8.3\n",
      "    Uninstalling numexpr-2.8.3:\n",
      "      Successfully uninstalled numexpr-2.8.3\n",
      "Successfully installed dataclasses-json-0.5.9 langchain-0.0.229 langchainplus-sdk-0.0.20 marshmallow-3.19.0 marshmallow-enum-1.5.1 numexpr-2.8.4 openapi-schema-pydantic-1.2.4 pydantic-1.10.11 typing-inspect-0.9.0\n"
     ]
    }
   ],
   "source": [
    "!pip install langchain\n",
    "\n",
    "import os\n",
    "import pandas as pd\n",
    "from langchain.embeddings import FakeEmbeddings\n",
    "from scipy import spatial\n",
    "\n",
    "import numpy as np\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "import pickle\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "70b4d986-7516-46f5-afeb-8732d68bc299",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_topk_similar_articles(embeddings, article_index, k, anti_similarity=False):\n",
    "    \"\"\"\n",
    "    Get the top k news articles with highest similarity for a given index\n",
    "    \n",
    "    embeddings: embeddings matrix\n",
    "    article_index: target article\n",
    "    k = get the top k articles\n",
    "    anti_similarity: if anti similar values should be used\n",
    "    return top articles for given index\n",
    "    \"\"\"\n",
    "\n",
    "    # Calculate pairwise cosine similarity with sklearn library\n",
    "    similarity_scores = cosine_similarity(embeddings)\n",
    "    \n",
    "    # get similarity scores for the given article index\n",
    "    article_scores = similarity_scores[article_index]\n",
    "        \n",
    "    # get anti_similarity or positive ones\n",
    "    # argsort returns the indices of the sorted array\n",
    "    if anti_similarity:\n",
    "        # [:k] to return only the top k article indices \n",
    "        sorted_indices = np.argsort(article_scores)[:k] \n",
    "    else:\n",
    "        # [::-1] to order in decending order\n",
    "        # 1:k+1 to exclude the target-article itself (similartiy = 1)\n",
    "        sorted_indices = np.argsort(article_scores)[::-1][1:k+1] \n",
    "    \n",
    "    # Get the top k (anti-) similar articles\n",
    "    top_articles = [(index, article_scores[index]) for index in sorted_indices if index != article_index]\n",
    " \n",
    "    return top_articles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fafc49b9-1338-4f69-885f-37baff73788c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculated_cosine_similarity(embeddings):\n",
    "    ''' Calculate pairwise similarities and return them '''\n",
    "    # Calculate pairwise cosine similarity with sklearn library\n",
    "    similarity_scores = cosine_similarity(embeddings)\n",
    "    return similarity_scores\n",
    "\n",
    "def get_topk_similar_articles_precalculated(similarity_scores, article_index, k, anti_similarity=False):\n",
    "    \"\"\"\n",
    "    Get the top k similar articles for a given article_index from the precomputed cos-similarties\n",
    "    \"\"\"\n",
    "    \n",
    "    # get similarity scores for the given article index\n",
    "    article_scores = similarity_scores[article_index]\n",
    "        \n",
    "    # get anti_similarity or positive ones\n",
    "    # argsort returns the indices of the sorted array\n",
    "    if anti_similarity:\n",
    "        # [:k] to return only the top k article indices \n",
    "        sorted_indices = np.argsort(article_scores)[:k] \n",
    "    else:\n",
    "        # [::-1] to order in decending order\n",
    "        # 1:k+1 to exclude the target-article itself (similartiy = 1)\n",
    "        sorted_indices = np.argsort(article_scores)[::-1][1:k+1] \n",
    "    \n",
    "    # Get the top k (anti-) similar articles\n",
    "    top_articles = [(index, article_scores[index]) for index in sorted_indices if index != article_index]\n",
    " \n",
    "    return top_articles\n",
    "\n",
    "def get_similarty_for_two_articles(similarity_scores, article_index, compare_index):\n",
    "    article_scores = similarity_scores[article_index]\n",
    "    return article_scores[compare_index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30514c02-0609-42da-ab54-30b1a1b4bead",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3bd0f071-26c7-492f-ae65-a66e9cab4c66",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5916\n",
      "2955\n"
     ]
    }
   ],
   "source": [
    "cleaned_articles = pd.read_csv(\"../data/cleaned_articles.csv\", index_col=False).drop(\"Unnamed: 0\", axis = 1)\n",
    "\n",
    "study_articles = pd.read_csv(\"../data/rating_similarity.csv\", index_col=False).drop(\"Unnamed: 0\", axis = 1)\n",
    "\n",
    "print(len(cleaned_articles))\n",
    "print(len(study_articles))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "056c842c-8062-46ae-8ec1-cd3852067ee0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>article_id</th>\n",
       "      <th>title</th>\n",
       "      <th>paragraphs</th>\n",
       "      <th>ressort</th>\n",
       "      <th>authors</th>\n",
       "      <th>date</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>FALTER_20151223BA00BC1175</td>\n",
       "      <td>DIE BLACK STREET BOYS</td>\n",
       "      <td>['Harald Mahrer: Der Nachdenkliche', 'Der Älte...</td>\n",
       "      <td>Politik</td>\n",
       "      <td>josef redl, barbara tóth</td>\n",
       "      <td>2015-12-23 00:00:00+00:00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                   article_id                  title  \\\n",
       "14  FALTER_20151223BA00BC1175  DIE BLACK STREET BOYS   \n",
       "\n",
       "                                           paragraphs  ressort  \\\n",
       "14  ['Harald Mahrer: Der Nachdenkliche', 'Der Älte...  Politik   \n",
       "\n",
       "                     authors                       date  \n",
       "14  josef redl, barbara tóth  2015-12-23 00:00:00+00:00  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cleaned_articles.loc[cleaned_articles['article_id'] == \"FALTER_20151223BA00BC1175\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ebc0567a-6013-4b98-b88d-3a6ecabdc03f",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Get article_id's of compared articles for a specific article \n",
    "'''\n",
    "def get_articles_for_base_article(base_article):\n",
    "    list = study_articles.loc[study_articles['article_id'] == base_article]\n",
    "    compared_article_ids = list.compared_article.values\n",
    "    return compared_article_ids\n",
    "\n",
    "\n",
    "# --------------------------------------------------------\n",
    "def get_index_for_articleid(article_id):\n",
    "    return cleaned_articles.loc[cleaned_articles['article_id'] == article_id].index.values[0]\n",
    "\n",
    "\n",
    "\n",
    "# --------------------------------------------------------\n",
    "def intersection(list_a, list_b):\n",
    "    return [ e for e in list_a if e in list_b ]\n",
    "\n",
    "\n",
    "\n",
    "# --------------------------------------------------------\n",
    "# get list of based-articles\n",
    "def get_base_articles():\n",
    "        list = study_articles['article_id']\n",
    "        return set(list)\n",
    "#for item in base_articles:\n",
    "#    falter_articles_ids = get_articles_for_base_article(item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8962b984-9c9d-4d68-987a-69554a616cbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def intersection_calculation(embeddings_file, embeddings_file2=0, use_second_embeddings=True, num_articles=15):\n",
    "    ''' Intersection calculation for embeddings with the labeled dataset '''\n",
    "    # -> calculate embeddings\n",
    "    # -> function to get index from article_id # get_index_for_articleid(article_id)\n",
    "    # -> base_articles # get_base_articles()\n",
    "    # -> compared_articles for a article()\n",
    "    # -> similarities from embeddings\n",
    "    # -> compare top 15 articles\n",
    "    \n",
    "    open_file = open(embeddings_file, \"rb\")\n",
    "    embeddings = pickle.load(open_file)\n",
    "    open_file.close()\n",
    "    similarity_scores = calculated_cosine_similarity(embeddings)\n",
    "    max_intersec = 0\n",
    "    num_same_articles = 0\n",
    "    \n",
    "    if use_second_embeddings:\n",
    "        base_articles = get_base_articles() # article_id of the articles that got compared\n",
    "\n",
    "        for article in base_articles:\n",
    "            labeled_dataset_articles = get_articles_for_base_article(article) # article ids for a base article\n",
    "            recommended_articles_with_sim = get_topk_similar_articles_precalculated(similarity_scores, get_index_for_articleid(article), num_articles, False)\n",
    "\n",
    "            indices_calculated = [item[0] for item in recommended_articles_with_sim]\n",
    "            indices_labled_dataset = [get_index_for_articleid(item) for item in labeled_dataset_articles]\n",
    "            intersec = intersection(indices_labled_dataset, indices_calculated)\n",
    "            num_same_articles += len(intersec)\n",
    "        \n",
    "        max_intersec = (len(base_articles)*15)\n",
    "\n",
    "    else:\n",
    "        open_file = open(embeddings_file2, \"rb\")\n",
    "        embeddings2 = pickle.load(open_file)\n",
    "        open_file.close()\n",
    "        similarity_scores2 = calculated_cosine_similarity(embeddings2)\n",
    "        \n",
    "        for i in range(5916):\n",
    "            recommended_articles_with_sim = get_topk_similar_articles_precalculated(similarity_scores, i, num_articles, False)\n",
    "            recommended_articles_with_sim2 = get_topk_similar_articles_precalculated(similarity_scores2, i, num_articles, False)\n",
    "            indices_calculated = [item[0] for item in recommended_articles_with_sim]\n",
    "            indices_calculated2 = [item[0] for item in recommended_articles_with_sim2]\n",
    "            intersec = intersection(indices_calculated, indices_calculated2)\n",
    "            num_same_articles += len(intersec)\n",
    "            \n",
    "        max_intersec = (5916*15)\n",
    "\n",
    "    print('Number of articles that are in in the intersection: ', num_same_articles)\n",
    "    coverage = \"Intersection in percentage: {:.2f}%\".format(100/max_intersec * num_same_articles)\n",
    "    print(coverage)\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9218a1ba-b881-42c0-98c7-ac826df3ed3a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OpenAI 80 Words:\n",
      "Number of articles that are in in the intersection:  467\n",
      "Intersection in percentage: 15.80%\n",
      "\n",
      "\n",
      "HuggingFace 80 Words:\n",
      "Number of articles that are in in the intersection:  251\n",
      "Intersection in percentage: 8.49%\n",
      "\n",
      "\n",
      "OpenAI 250 Words:\n",
      "Number of articles that are in in the intersection:  619\n",
      "Intersection in percentage: 20.95%\n",
      "\n",
      "\n",
      "HuggingFace 250 Words:\n",
      "Number of articles that are in in the intersection:  404\n",
      "Intersection in percentage: 13.67%\n",
      "\n",
      "\n",
      "OpenAI 80 words vs OpenAI 250 words with 15 articles:\n",
      "Number of articles that are in in the intersection:  49964\n",
      "Intersection in percentage: 56.30%\n",
      "\n",
      "\n",
      "OpenAI 80 words vs OpenAI 250 words with 25 articles:\n",
      "Number of articles that are in in the intersection:  85468\n",
      "Intersection in percentage: 96.31%\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#197 Base articles => 197 * 15 = 2955 articles in theory \n",
    "\n",
    "print('OpenAI 80 Words:')\n",
    "intersection_calculation(\"../data/openaiembed.pkl\")\n",
    "print('HuggingFace 80 Words:')\n",
    "intersection_calculation(\"../data/huggingface_embeddings.pkl\")\n",
    "print('OpenAI 250 Words:')\n",
    "intersection_calculation(\"../data/openaiembed_top250.pkl\")\n",
    "print('HuggingFace 250 Words:')\n",
    "intersection_calculation(\"../data/huggingface_top250_embedding.pkl\")\n",
    "\n",
    "print('OpenAI 80 words vs OpenAI 250 words with 15 articles:')\n",
    "intersection_calculation(\"../data/openaiembed_top250.pkl\", \"../data/openaiembed.pkl\", False)\n",
    "\n",
    "print('OpenAI 80 words vs OpenAI 250 words with 25 articles:')\n",
    "intersection_calculation(\"../data/openaiembed_top250.pkl\", \"../data/openaiembed.pkl\", False, 25)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b4a535af-a9b8-4208-a0b2-6057e15b4f07",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def get_min_max_similarties(filename):\n",
    "    open_file = open(filename, \"rb\")\n",
    "    embeddings = pickle.load(open_file)\n",
    "    open_file.close()\n",
    "\n",
    "    similarity_scores = calculated_cosine_similarity(embeddings)\n",
    "\n",
    "    # def get_topk_similar_articles_precalculated(similarity_scores, article_index, k, negativSimilartiy=False):\n",
    "\n",
    "\n",
    "    min_sim = 1\n",
    "    max_sim = 0\n",
    "    for i in range(5916):\n",
    "        list = get_topk_similar_articles_precalculated(similarity_scores, i, 1, True)\n",
    "        sim = list[0][1]\n",
    "        min_sim = min(min_sim, sim)\n",
    "        list = get_topk_similar_articles_precalculated(similarity_scores, i, 2, False)\n",
    "        sim = list[0][1]\n",
    "        if sim < 1:\n",
    "            max_sim = max(max_sim, sim)    \n",
    "\n",
    "    print('Minimal cos-sim for all articles: ', min_sim)\n",
    "    print('Maximal cos-sim for all articles: ', max_sim)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "449b4548-417a-49de-97d2-85b158e5f23f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OpenAI Top80 Embeddings: \n",
      "Minimal cos-sim for all articles:  0.7296835164144966\n",
      "Maximal cos-sim for all articles:  0.9974947860618575\n",
      "\n",
      "OpenAI Top250 Embeddings: \n",
      "Minimal cos-sim for all articles:  0.6979026654793344\n",
      "Maximal cos-sim for all articles:  0.9999999999999998\n",
      "\n",
      "HuggingFace Top80 Embeddings: \n",
      "Minimal cos-sim for all articles:  -0.11489994902211896\n",
      "Maximal cos-sim for all articles:  0.9975477057357242\n",
      "\n",
      "HuggingFace Top250 Embeddings: \n",
      "Minimal cos-sim for all articles:  0.26555460047357027\n",
      "Maximal cos-sim for all articles:  0.999478314915158\n"
     ]
    }
   ],
   "source": [
    "print(\"OpenAI Top80 Embeddings: \")\n",
    "get_min_max_similarties(\"../data/openaiembed_top250.pkl\")\n",
    "\n",
    "print(\"\\nOpenAI Top250 Embeddings: \")\n",
    "get_min_max_similarties(\"../data/openaiembed.pkl\")\n",
    "\n",
    "print(\"\\nHuggingFace Top80 Embeddings: \")\n",
    "get_min_max_similarties(\"../data/huggingface_embeddings.pkl\")\n",
    "\n",
    "print(\"\\nHuggingFace Top250 Embeddings: \")\n",
    "get_min_max_similarties(\"../data/huggingface_top250_embedding.pkl\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
