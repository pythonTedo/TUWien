{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a33c2a98-ac0e-43e2-a608-1a9300dce9bc",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting nltk\n",
      "  Using cached nltk-3.8.1-py3-none-any.whl (1.5 MB)\n",
      "Requirement already satisfied: click in /opt/conda/lib/python3.10/site-packages (from nltk) (8.1.3)\n",
      "Requirement already satisfied: tqdm in /opt/conda/lib/python3.10/site-packages (from nltk) (4.64.1)\n",
      "Requirement already satisfied: regex>=2021.8.3 in /opt/conda/lib/python3.10/site-packages (from nltk) (2022.10.31)\n",
      "Requirement already satisfied: joblib in /opt/conda/lib/python3.10/site-packages (from nltk) (1.2.0)\n",
      "Installing collected packages: nltk\n",
      "Successfully installed nltk-3.8.1\n",
      "Collecting langchain\n",
      "  Downloading langchain-0.0.230-py3-none-any.whl (1.3 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m20.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: async-timeout<5.0.0,>=4.0.0 in /opt/conda/lib/python3.10/site-packages (from langchain) (4.0.2)\n",
      "Collecting numexpr<3.0.0,>=2.8.4\n",
      "  Using cached numexpr-2.8.4-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (381 kB)\n",
      "Requirement already satisfied: numpy<2,>=1 in /opt/conda/lib/python3.10/site-packages (from langchain) (1.23.5)\n",
      "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /opt/conda/lib/python3.10/site-packages (from langchain) (2.0.1)\n",
      "Collecting langchainplus-sdk<0.0.21,>=0.0.20\n",
      "  Using cached langchainplus_sdk-0.0.20-py3-none-any.whl (25 kB)\n",
      "Requirement already satisfied: tenacity<9.0.0,>=8.1.0 in /opt/conda/lib/python3.10/site-packages (from langchain) (8.2.2)\n",
      "Requirement already satisfied: requests<3,>=2 in /opt/conda/lib/python3.10/site-packages (from langchain) (2.28.2)\n",
      "Requirement already satisfied: PyYAML>=5.4.1 in /opt/conda/lib/python3.10/site-packages (from langchain) (6.0)\n",
      "Collecting openapi-schema-pydantic<2.0,>=1.2\n",
      "  Using cached openapi_schema_pydantic-1.2.4-py3-none-any.whl (90 kB)\n",
      "Collecting dataclasses-json<0.6.0,>=0.5.7\n",
      "  Using cached dataclasses_json-0.5.9-py3-none-any.whl (26 kB)\n",
      "Collecting pydantic<2,>=1\n",
      "  Using cached pydantic-1.10.11-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.1 MB)\n",
      "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /opt/conda/lib/python3.10/site-packages (from langchain) (3.8.4)\n",
      "Requirement already satisfied: charset-normalizer<4.0,>=2.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (2.1.1)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /opt/conda/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.3.3)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.8.2)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (22.2.0)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /opt/conda/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.3.1)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /opt/conda/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (6.0.4)\n",
      "Collecting marshmallow<4.0.0,>=3.3.0\n",
      "  Using cached marshmallow-3.19.0-py3-none-any.whl (49 kB)\n",
      "Collecting marshmallow-enum<2.0.0,>=1.5.1\n",
      "  Using cached marshmallow_enum-1.5.1-py2.py3-none-any.whl (4.2 kB)\n",
      "Collecting typing-inspect>=0.4.0\n",
      "  Using cached typing_inspect-0.9.0-py3-none-any.whl (8.8 kB)\n",
      "Requirement already satisfied: typing-extensions>=4.2.0 in /opt/conda/lib/python3.10/site-packages (from pydantic<2,>=1->langchain) (4.4.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests<3,>=2->langchain) (2022.12.7)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests<3,>=2->langchain) (3.4)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests<3,>=2->langchain) (1.26.14)\n",
      "Requirement already satisfied: greenlet!=0.4.17 in /opt/conda/lib/python3.10/site-packages (from SQLAlchemy<3,>=1.4->langchain) (2.0.2)\n",
      "Requirement already satisfied: packaging>=17.0 in /opt/conda/lib/python3.10/site-packages (from marshmallow<4.0.0,>=3.3.0->dataclasses-json<0.6.0,>=0.5.7->langchain) (23.0)\n",
      "Requirement already satisfied: mypy-extensions>=0.3.0 in /opt/conda/lib/python3.10/site-packages (from typing-inspect>=0.4.0->dataclasses-json<0.6.0,>=0.5.7->langchain) (1.0.0)\n",
      "Installing collected packages: typing-inspect, pydantic, numexpr, marshmallow, openapi-schema-pydantic, marshmallow-enum, langchainplus-sdk, dataclasses-json, langchain\n",
      "  Attempting uninstall: numexpr\n",
      "    Found existing installation: numexpr 2.8.3\n",
      "    Uninstalling numexpr-2.8.3:\n",
      "      Successfully uninstalled numexpr-2.8.3\n",
      "Successfully installed dataclasses-json-0.5.9 langchain-0.0.230 langchainplus-sdk-0.0.20 marshmallow-3.19.0 marshmallow-enum-1.5.1 numexpr-2.8.4 openapi-schema-pydantic-1.2.4 pydantic-1.10.11 typing-inspect-0.9.0\n",
      "Collecting sentence_transformers\n",
      "  Using cached sentence_transformers-2.2.2-py3-none-any.whl\n",
      "Requirement already satisfied: scikit-learn in /opt/conda/lib/python3.10/site-packages (from sentence_transformers) (1.2.2)\n",
      "Collecting transformers<5.0.0,>=4.6.0\n",
      "  Using cached transformers-4.30.2-py3-none-any.whl (7.2 MB)\n",
      "Requirement already satisfied: tqdm in /opt/conda/lib/python3.10/site-packages (from sentence_transformers) (4.64.1)\n",
      "Collecting huggingface-hub>=0.4.0\n",
      "  Downloading huggingface_hub-0.16.4-py3-none-any.whl (268 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m268.8/268.8 kB\u001b[0m \u001b[31m9.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting torch>=1.6.0\n",
      "  Using cached torch-2.0.1-cp310-cp310-manylinux1_x86_64.whl (619.9 MB)\n",
      "Collecting torchvision\n",
      "  Using cached torchvision-0.15.2-cp310-cp310-manylinux1_x86_64.whl (6.0 MB)\n",
      "Requirement already satisfied: scipy in /opt/conda/lib/python3.10/site-packages (from sentence_transformers) (1.10.1)\n",
      "Collecting sentencepiece\n",
      "  Using cached sentencepiece-0.1.99-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
      "Requirement already satisfied: nltk in /opt/conda/lib/python3.10/site-packages (from sentence_transformers) (3.8.1)\n",
      "Requirement already satisfied: numpy in /opt/conda/lib/python3.10/site-packages (from sentence_transformers) (1.23.5)\n",
      "Requirement already satisfied: packaging>=20.9 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.4.0->sentence_transformers) (23.0)\n",
      "Requirement already satisfied: fsspec in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.4.0->sentence_transformers) (2023.3.0)\n",
      "Collecting filelock\n",
      "  Using cached filelock-3.12.2-py3-none-any.whl (10 kB)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.4.0->sentence_transformers) (6.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.4.0->sentence_transformers) (4.4.0)\n",
      "Requirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.4.0->sentence_transformers) (2.28.2)\n",
      "Collecting nvidia-curand-cu11==10.2.10.91\n",
      "  Using cached nvidia_curand_cu11-10.2.10.91-py3-none-manylinux1_x86_64.whl (54.6 MB)\n",
      "Collecting triton==2.0.0\n",
      "  Using cached triton-2.0.0-1-cp310-cp310-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (63.3 MB)\n",
      "Collecting nvidia-cusolver-cu11==11.4.0.1\n",
      "  Using cached nvidia_cusolver_cu11-11.4.0.1-2-py3-none-manylinux1_x86_64.whl (102.6 MB)\n",
      "Requirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch>=1.6.0->sentence_transformers) (3.0)\n",
      "Collecting nvidia-cuda-runtime-cu11==11.7.99\n",
      "  Using cached nvidia_cuda_runtime_cu11-11.7.99-py3-none-manylinux1_x86_64.whl (849 kB)\n",
      "Collecting nvidia-cublas-cu11==11.10.3.66\n",
      "  Using cached nvidia_cublas_cu11-11.10.3.66-py3-none-manylinux1_x86_64.whl (317.1 MB)\n",
      "Collecting nvidia-cusparse-cu11==11.7.4.91\n",
      "  Using cached nvidia_cusparse_cu11-11.7.4.91-py3-none-manylinux1_x86_64.whl (173.2 MB)\n",
      "Collecting nvidia-nccl-cu11==2.14.3\n",
      "  Using cached nvidia_nccl_cu11-2.14.3-py3-none-manylinux1_x86_64.whl (177.1 MB)\n",
      "Collecting nvidia-nvtx-cu11==11.7.91\n",
      "  Using cached nvidia_nvtx_cu11-11.7.91-py3-none-manylinux1_x86_64.whl (98 kB)\n",
      "Requirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch>=1.6.0->sentence_transformers) (3.1.2)\n",
      "Collecting nvidia-cuda-nvrtc-cu11==11.7.99\n",
      "  Using cached nvidia_cuda_nvrtc_cu11-11.7.99-2-py3-none-manylinux1_x86_64.whl (21.0 MB)\n",
      "Collecting nvidia-cuda-cupti-cu11==11.7.101\n",
      "  Using cached nvidia_cuda_cupti_cu11-11.7.101-py3-none-manylinux1_x86_64.whl (11.8 MB)\n",
      "Collecting nvidia-cudnn-cu11==8.5.0.96\n",
      "  Using cached nvidia_cudnn_cu11-8.5.0.96-2-py3-none-manylinux1_x86_64.whl (557.1 MB)\n",
      "Requirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch>=1.6.0->sentence_transformers) (1.11.1)\n",
      "Collecting nvidia-cufft-cu11==10.9.0.58\n",
      "  Using cached nvidia_cufft_cu11-10.9.0.58-py3-none-manylinux1_x86_64.whl (168.4 MB)\n",
      "Requirement already satisfied: wheel in /opt/conda/lib/python3.10/site-packages (from nvidia-cublas-cu11==11.10.3.66->torch>=1.6.0->sentence_transformers) (0.38.4)\n",
      "Requirement already satisfied: setuptools in /opt/conda/lib/python3.10/site-packages (from nvidia-cublas-cu11==11.10.3.66->torch>=1.6.0->sentence_transformers) (67.1.0)\n",
      "Collecting lit\n",
      "  Using cached lit-16.0.6-py3-none-any.whl\n",
      "Collecting cmake\n",
      "  Using cached cmake-3.26.4-py2.py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (24.0 MB)\n",
      "Collecting safetensors>=0.3.1\n",
      "  Using cached safetensors-0.3.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
      "Collecting tokenizers!=0.11.3,<0.14,>=0.11.1\n",
      "  Using cached tokenizers-0.13.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (7.8 MB)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.10/site-packages (from transformers<5.0.0,>=4.6.0->sentence_transformers) (2022.10.31)\n",
      "Requirement already satisfied: click in /opt/conda/lib/python3.10/site-packages (from nltk->sentence_transformers) (8.1.3)\n",
      "Requirement already satisfied: joblib in /opt/conda/lib/python3.10/site-packages (from nltk->sentence_transformers) (1.2.0)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /opt/conda/lib/python3.10/site-packages (from scikit-learn->sentence_transformers) (3.1.0)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /opt/conda/lib/python3.10/site-packages (from torchvision->sentence_transformers) (9.2.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch>=1.6.0->sentence_transformers) (2.1.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub>=0.4.0->sentence_transformers) (3.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub>=0.4.0->sentence_transformers) (2022.12.7)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub>=0.4.0->sentence_transformers) (1.26.14)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub>=0.4.0->sentence_transformers) (2.1.1)\n",
      "Requirement already satisfied: mpmath>=0.19 in /opt/conda/lib/python3.10/site-packages (from sympy->torch>=1.6.0->sentence_transformers) (1.3.0)\n",
      "Installing collected packages: tokenizers, sentencepiece, safetensors, lit, cmake, nvidia-nvtx-cu11, nvidia-nccl-cu11, nvidia-cusparse-cu11, nvidia-curand-cu11, nvidia-cufft-cu11, nvidia-cuda-runtime-cu11, nvidia-cuda-nvrtc-cu11, nvidia-cuda-cupti-cu11, nvidia-cublas-cu11, filelock, nvidia-cusolver-cu11, nvidia-cudnn-cu11, huggingface-hub, transformers, triton, torch, torchvision, sentence_transformers\n",
      "^C\n",
      "\u001b[31mERROR: Operation cancelled by user\u001b[0m\u001b[31m\n",
      "\u001b[0mCollecting openai\n",
      "  Using cached openai-0.27.8-py3-none-any.whl (73 kB)\n",
      "Requirement already satisfied: requests>=2.20 in /opt/conda/lib/python3.10/site-packages (from openai) (2.28.2)\n",
      "Requirement already satisfied: tqdm in /opt/conda/lib/python3.10/site-packages (from openai) (4.64.1)\n",
      "Requirement already satisfied: aiohttp in /opt/conda/lib/python3.10/site-packages (from openai) (3.8.4)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests>=2.20->openai) (2.1.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests>=2.20->openai) (3.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests>=2.20->openai) (2022.12.7)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests>=2.20->openai) (1.26.14)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /opt/conda/lib/python3.10/site-packages (from aiohttp->openai) (6.0.4)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->openai) (1.8.2)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /opt/conda/lib/python3.10/site-packages (from aiohttp->openai) (1.3.1)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /opt/conda/lib/python3.10/site-packages (from aiohttp->openai) (4.0.2)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /opt/conda/lib/python3.10/site-packages (from aiohttp->openai) (1.3.3)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->openai) (22.2.0)\n",
      "Installing collected packages: openai\n",
      "Successfully installed openai-0.27.8\n",
      "Collecting tiktoken\n",
      "  Using cached tiktoken-0.4.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.7 MB)\n",
      "Requirement already satisfied: regex>=2022.1.18 in /opt/conda/lib/python3.10/site-packages (from tiktoken) (2022.10.31)\n",
      "Requirement already satisfied: requests>=2.26.0 in /opt/conda/lib/python3.10/site-packages (from tiktoken) (2.28.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests>=2.26.0->tiktoken) (3.4)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests>=2.26.0->tiktoken) (1.26.14)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests>=2.26.0->tiktoken) (2.1.1)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests>=2.26.0->tiktoken) (2022.12.7)\n",
      "Installing collected packages: tiktoken\n",
      "Successfully installed tiktoken-0.4.0\n"
     ]
    }
   ],
   "source": [
    "!pip install nltk\n",
    "!pip install langchain\n",
    "!pip install sentence_transformers\n",
    "!pip install openai\n",
    "!pip install tiktoken"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "03ddfe77-2d95-40b2-ab3f-d6d581d11c7b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /home/e11709501/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import nltk \n",
    "import nltk as nlp\n",
    "nltk.download('stopwords')\n",
    "import re\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import pickle\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0f5e6422-71c9-4b0e-896e-06b7f3fd0225",
   "metadata": {},
   "outputs": [],
   "source": [
    "stop = set(stopwords.words(\"german\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "19bccd03-963b-44a0-90e1-15092b133b79",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '../data/combined.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m df \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mread_csv(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m../data/articles_2015.csv\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m----> 3\u001b[0m comb \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_csv\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m../data/combined.csv\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/pandas/util/_decorators.py:211\u001b[0m, in \u001b[0;36mdeprecate_kwarg.<locals>._deprecate_kwarg.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    209\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    210\u001b[0m         kwargs[new_arg_name] \u001b[38;5;241m=\u001b[39m new_arg_value\n\u001b[0;32m--> 211\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/pandas/util/_decorators.py:331\u001b[0m, in \u001b[0;36mdeprecate_nonkeyword_arguments.<locals>.decorate.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    325\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(args) \u001b[38;5;241m>\u001b[39m num_allow_args:\n\u001b[1;32m    326\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[1;32m    327\u001b[0m         msg\u001b[38;5;241m.\u001b[39mformat(arguments\u001b[38;5;241m=\u001b[39m_format_argument_list(allow_args)),\n\u001b[1;32m    328\u001b[0m         \u001b[38;5;167;01mFutureWarning\u001b[39;00m,\n\u001b[1;32m    329\u001b[0m         stacklevel\u001b[38;5;241m=\u001b[39mfind_stack_level(),\n\u001b[1;32m    330\u001b[0m     )\n\u001b[0;32m--> 331\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/pandas/io/parsers/readers.py:950\u001b[0m, in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, error_bad_lines, warn_bad_lines, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options)\u001b[0m\n\u001b[1;32m    935\u001b[0m kwds_defaults \u001b[38;5;241m=\u001b[39m _refine_defaults_read(\n\u001b[1;32m    936\u001b[0m     dialect,\n\u001b[1;32m    937\u001b[0m     delimiter,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    946\u001b[0m     defaults\u001b[38;5;241m=\u001b[39m{\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdelimiter\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m,\u001b[39m\u001b[38;5;124m\"\u001b[39m},\n\u001b[1;32m    947\u001b[0m )\n\u001b[1;32m    948\u001b[0m kwds\u001b[38;5;241m.\u001b[39mupdate(kwds_defaults)\n\u001b[0;32m--> 950\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_read\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/pandas/io/parsers/readers.py:605\u001b[0m, in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    602\u001b[0m _validate_names(kwds\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnames\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m))\n\u001b[1;32m    604\u001b[0m \u001b[38;5;66;03m# Create the parser.\u001b[39;00m\n\u001b[0;32m--> 605\u001b[0m parser \u001b[38;5;241m=\u001b[39m \u001b[43mTextFileReader\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    607\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m chunksize \u001b[38;5;129;01mor\u001b[39;00m iterator:\n\u001b[1;32m    608\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/pandas/io/parsers/readers.py:1442\u001b[0m, in \u001b[0;36mTextFileReader.__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m   1439\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m kwds[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m   1441\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles: IOHandles \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m-> 1442\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_engine \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_make_engine\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mengine\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/pandas/io/parsers/readers.py:1735\u001b[0m, in \u001b[0;36mTextFileReader._make_engine\u001b[0;34m(self, f, engine)\u001b[0m\n\u001b[1;32m   1733\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m mode:\n\u001b[1;32m   1734\u001b[0m         mode \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m-> 1735\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;241m=\u001b[39m \u001b[43mget_handle\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1736\u001b[0m \u001b[43m    \u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1737\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1738\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mencoding\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1739\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcompression\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcompression\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1740\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmemory_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmemory_map\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1741\u001b[0m \u001b[43m    \u001b[49m\u001b[43mis_text\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_text\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1742\u001b[0m \u001b[43m    \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mencoding_errors\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstrict\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1743\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstorage_options\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1744\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1745\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1746\u001b[0m f \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles\u001b[38;5;241m.\u001b[39mhandle\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/pandas/io/common.py:856\u001b[0m, in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    851\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(handle, \u001b[38;5;28mstr\u001b[39m):\n\u001b[1;32m    852\u001b[0m     \u001b[38;5;66;03m# Check whether the filename is to be opened in binary mode.\u001b[39;00m\n\u001b[1;32m    853\u001b[0m     \u001b[38;5;66;03m# Binary mode does not support 'encoding' and 'newline'.\u001b[39;00m\n\u001b[1;32m    854\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mencoding \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mmode:\n\u001b[1;32m    855\u001b[0m         \u001b[38;5;66;03m# Encoding\u001b[39;00m\n\u001b[0;32m--> 856\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[1;32m    857\u001b[0m \u001b[43m            \u001b[49m\u001b[43mhandle\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    858\u001b[0m \u001b[43m            \u001b[49m\u001b[43mioargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    859\u001b[0m \u001b[43m            \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mioargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencoding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    860\u001b[0m \u001b[43m            \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43merrors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    861\u001b[0m \u001b[43m            \u001b[49m\u001b[43mnewline\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    862\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    863\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    864\u001b[0m         \u001b[38;5;66;03m# Binary mode\u001b[39;00m\n\u001b[1;32m    865\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mopen\u001b[39m(handle, ioargs\u001b[38;5;241m.\u001b[39mmode)\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '../data/combined.csv'"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv(\"../data/articles_2015.csv\")\n",
    "\n",
    "comb = pd.read_csv(\"../data/combined.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a456f36-1bb1-4868-b7b1-656d54c9c3a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "comb"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78e3d98c-1944-40f7-868a-8d3eb78c6d74",
   "metadata": {},
   "source": [
    "## Data Selection and Dataframe merge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0ba5421-622e-4a60-bc1e-614542fe5a33",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "721e7f81-9a21-4ecd-ada3-18e0324aa448",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03ca67db-9b3f-4c2a-b3c3-6a2461e7d068",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "589f8ab3-814b-403c-b51f-dfe13414b000",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.authors[df.authors == \"[]\"].count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff0581eb-82e3-4d86-b778-221395c30411",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.ressort.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7c19299-32f3-41a5-97bc-5800dd5aaa43",
   "metadata": {},
   "outputs": [],
   "source": [
    "newdf = df[[\"article_id\", \"title\", \"paragraphs\", \"ressort\", \"authors\", 'date']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfb91c14-3cfc-48e5-9c88-4a6e604a90e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "newdf.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3541cfdc-fc13-482c-8595-30dd84b980a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "newdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c39b383-df18-45a6-a94f-166359149ecc",
   "metadata": {},
   "outputs": [],
   "source": [
    "comb = comb.rename(columns={\"target_article\": \"article_id\", \"article_id\":\"compared_article\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4c68b40-b2b0-4168-bddd-39cc1d88cb1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "comb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdf9224b-0b08-4fd7-8842-3a581cfd8f9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "merged = newdf.merge(right=comb, how=\"left\", on=\"article_id\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4eae64f-c22e-421d-a0ac-7e6549f08e83",
   "metadata": {},
   "outputs": [],
   "source": [
    "comb.article_id.nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11fe8458-0ac0-40fd-86d4-acc370b9173f",
   "metadata": {},
   "outputs": [],
   "source": [
    "merged[merged.article_id == \"FALTER_20151223BA00BC1175\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a2244fe-3777-473a-866d-822937f66317",
   "metadata": {},
   "source": [
    "## Data Exploration and Viz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "368fa1cc-06eb-4024-aa3f-98f629e3d0b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "merged.ressort.value_counts().plot(kind=\"bar\", title = \"Ressort types\", ylabel = \"Frequency\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bbd190c-b5e3-4e01-b886-3c9cc7db6617",
   "metadata": {},
   "outputs": [],
   "source": [
    "merged.compared_article.value_counts().plot(kind=\"hist\", bins=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54e8dc3d-7244-4742-bf2c-c9fb733a936a",
   "metadata": {},
   "outputs": [],
   "source": [
    "merged.compared_article.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "056c9b49-4815-4837-a6a8-c936bb4b30ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "merged[merged.compared_article==\"FALTER_20150408A2F8406B8A\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc14f7ec-7b30-4994-907b-ea241f9239fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "merged[merged.compared_article==\"FALTER_2015020483BDE5657B\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70fad5e8-b744-4c8d-883d-9529d1f649f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "merged[merged.compared_article==\"FALTER_2015110460C1FE5811\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b901ecc8-0cfa-4528-9ba8-83cd21db24f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt_sim_hit = merged.Similarity.hist(bins = 13)\n",
    "plt_sim_hit.set_xlabel(\"Similarity values\")\n",
    "plt_sim_hit.set_title(\"Frequency of the similarity values\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f9aa48a-629c-43d8-b8c3-73561f3be065",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d71c3696-8f42-4bb0-9c85-4c5e563fc901",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "122faefb-6e03-4423-be84-11cca909be24",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d851bdc2-2584-433a-9b0d-bde48ccf8521",
   "metadata": {},
   "outputs": [],
   "source": [
    "#####################################NEW##########################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "437c9cf6-96a7-4a71-99b4-75eb65f4e0f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "merged.authors.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "965a563e-83ca-45c1-b893-8c31253d7a06",
   "metadata": {},
   "outputs": [],
   "source": [
    "merged['authors'] = merged['authors'].str.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10ba57e5-c3ea-4129-bf10-33c8c2d401ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(0, len(merged)):\n",
    "    if \":\" in merged.authors[i]:\n",
    "        merged.authors[i] = merged.authors[i].split(\":\")[-1].strip()[:-2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f37f7ec-c15b-4f17-9d85-da8d0edf16a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "merged.authors = merged.authors.str.replace(\"[\", \"\").str.replace(\"]\", \"\").str.replace(\"'\", \"\").str.replace(\" und\", \",\").str.replace(\" &\", \",\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61dce49e-b932-4635-b4da-d4ca335657a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_names = ','.join(merged['authors'])\n",
    "\n",
    "all_names_list = all_names.split(',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ca2f16b-b7ed-435a-a481-128b7afc3494",
   "metadata": {},
   "outputs": [],
   "source": [
    "name_counts = pd.Series(all_names_list).value_counts()\n",
    "name_counts = name_counts.iloc[1:]\n",
    "name_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc7d6440-e20e-4007-8a05-844565e9554f",
   "metadata": {},
   "outputs": [],
   "source": [
    "name_counts_top = name_counts.iloc[:20]\n",
    "name_counts_top.plot(kind='bar')\n",
    "\n",
    "plt.xlabel('Names')\n",
    "plt.ylabel('Count')\n",
    "plt.title('Top 20 Count of Unique Names')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "875f5142-5a6f-4e55-a358-89b17242a610",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f090e8e4-c6e4-424e-ba85-acf171ab4a46",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e470d304-dc06-46b6-b504-4aec275523b8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d161616f-b86f-400b-b824-3612e2d7b9f5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "018ae5bc-0e38-46eb-8f81-4dd0bdbe8e53",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "7360a388-9eb5-42ff-9df3-78839b9f8316",
   "metadata": {},
   "source": [
    "## Data Cleaining and Text prep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e62b0ea3-97e1-458a-9eb2-b830e25a764a",
   "metadata": {},
   "outputs": [],
   "source": [
    "dftext = pd.DataFrame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a12acf04-a72a-4b38-83dd-1295fc5c6edd",
   "metadata": {},
   "outputs": [],
   "source": [
    "merged.text = merged.paragraphs+ \" \"+merged.title + \" \" + merged.ressort + \" \" + merged.authors`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d4c595f-11c0-480b-a0c4-9deba94cb1a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "merged.text = merged.text.fillna(\" \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe398dd7-a293-4e3e-bf82-35b69766590b",
   "metadata": {},
   "outputs": [],
   "source": [
    "merged.text.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6183af68-43ce-4b93-94fc-b64045f53120",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean(review):\n",
    "    if isinstance(review, str):\n",
    "        review = review.lower()\n",
    "        review = re.sub('[^a-z A-Z 0-9-]+', '', review)\n",
    "        review = \" \".join([word for word in review.split() if word not in stopwords.words('german')])\n",
    "        \n",
    "        return review"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9d448e9-1446-4bb5-9f44-b1c5c5349df6",
   "metadata": {},
   "outputs": [],
   "source": [
    "merged.text = merged.text.apply(clean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61ab8623-376a-4dc4-b620-63bba06a71d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "merged.text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e20c6520-c87e-4f0c-aa03-acab60a20577",
   "metadata": {},
   "outputs": [],
   "source": [
    "merged.text[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce880491-e5e6-4a32-b395-f3b7a19ddecd",
   "metadata": {},
   "outputs": [],
   "source": [
    "merged.to_csv(\"../data/cleaned.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5b4280f-7411-4ac3-800e-ab5fe2d42e37",
   "metadata": {},
   "outputs": [],
   "source": [
    "merged[\"text\"] = merged.text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57b037f9-58cc-4450-9a90-bd2e82c17788",
   "metadata": {},
   "outputs": [],
   "source": [
    "merged"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da4d9117-08fc-40e2-a309-d417cc25653b",
   "metadata": {},
   "source": [
    "Get Top Most important words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "862e9986-5350-4895-b1df-27c966f1b77c",
   "metadata": {},
   "outputs": [],
   "source": [
    "newdf[\"text\"] = newdf.paragraphs + \" \" + newdf.title + \" \" + newdf.ressort + \" \" + newdf.authors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec743c3f-33ce-484b-9e0a-adcab5f5d070",
   "metadata": {},
   "outputs": [],
   "source": [
    "newdf[\"text\"] = newdf[\"text\"].fillna(\" \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27190705-2f15-4a7e-8930-12cd397ecb37",
   "metadata": {},
   "outputs": [],
   "source": [
    "newdf[\"text\"].isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "592e4a18-8dc0-4000-8122-8143405bcd38",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "newdf.text = newdf.text.apply(clean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a29ee41-9c04-4f06-a644-0a9eab103171",
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = TfidfVectorizer()\n",
    "tfidf_matrix = vectorizer.fit_transform(newdf.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa05f6c9-a926-44e2-9321-9d2750eb3d6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "newdf.text.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d24f2db1-805f-44c5-b0bb-7aa2629354f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "newdf.title.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5224729c-a536-4b75-8e1a-4f212a8618e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "newdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "377d783a-420b-4a87-b5c1-bf54e5638b12",
   "metadata": {},
   "outputs": [],
   "source": [
    "newdf.text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f2b62c1-a4af-41c3-aae7-11b20046ca05",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retrieve feature names\n",
    "feature_names = vectorizer.get_feature_names_out()\n",
    "\n",
    "# Get IDF scores\n",
    "idf_scores = vectorizer.idf_\n",
    "\n",
    "# Combine feature names and IDF scores\n",
    "features_with_scores = list(zip(feature_names, idf_scores))\n",
    "\n",
    "# Sort the list based on IDF scores\n",
    "sorted_features = sorted(features_with_scores, key=lambda x: x[1], reverse=True)\n",
    "\n",
    "# Specify the number of top words to retrieve\n",
    "top_n = 100\n",
    "\n",
    "# Retrieve the top N words\n",
    "top_words = sorted_features[:top_n]\n",
    "\n",
    "# Print the top words and their IDF scores\n",
    "for word, score in top_words:\n",
    "    print(f\"Word: {word}, IDF Score: {score}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0ab8a4c-0aa6-4152-af86-2ce06ba9fe0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_names = vectorizer.get_feature_names_out()\n",
    "\n",
    "top_n = 80"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2835b90-64c7-4135-b105-9ae147e3c673",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an empty list to store the top words for each text\n",
    "top_words_per_text = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a94a3ef-a1a5-4917-a156-2dfca46c7ed2",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, text in enumerate(newdf.text):\n",
    "    # Transform the text into a TF-IDF vector\n",
    "    tfidf_vector = vectorizer.transform([text])\n",
    "\n",
    "    # Get the TF-IDF scores for the text\n",
    "    tfidf_scores = zip(feature_names, tfidf_vector.toarray()[0])\n",
    "\n",
    "    # Sort the TF-IDF scores in descending order\n",
    "    sorted_scores = sorted(tfidf_scores, key=lambda x: x[1], reverse=True)\n",
    "\n",
    "    # Retrieve the top N words for the text\n",
    "    top_words = [word for word, score in sorted_scores[:top_n]]\n",
    "\n",
    "    # Store the top words for the text in the dictionary\n",
    "    \n",
    "        # Combine the top words into a single string\n",
    "    top_words_text = ' '.join(top_words)\n",
    "\n",
    "    # Store the top words for the text in the list\n",
    "    top_words_per_text.append(top_words_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fddf356d-9026-4180-afe0-4593d5d72123",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_picked = pd.DataFrame({'Top_Words': top_words_per_text})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "755381d2-184c-4c2b-a4ce-58dcf435ca0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_picked"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "245450ee-fb92-40fe-a960-0751492efb20",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_picked[\"article_id\"] = newdf.article_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a116205e-7f38-4848-b70a-9b2181031622",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_picked"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "626b17b6-ea20-4e69-99dc-724409fc0678",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_picked.to_csv(\"top_80.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c37dc31-41ae-446b-9ac2-b59a89921bf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_picked.Top_Words[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5eccd9f-a56a-4956-adbd-ffcebe181e99",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "embeddings = HuggingFaceEmbeddings(model_name=\"bert-base-german-cased\")\n",
    "\n",
    "query_result = embeddings.embed_documents(df_picked.Top_Words)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec54d298-dddc-484f-930f-067ca315af63",
   "metadata": {},
   "source": [
    "# df_picked.Top_Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60150c56-a72e-4213-82ac-5b2319c6bd94",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "def get_topk_similar_articles(embeddings, article_index, k, negativSimilartiy=False):\n",
    "    \"\"\"\n",
    "    Get the top k news articles with highest similarity for a given index\n",
    "    \n",
    "    embeddings: embeddings matrix\n",
    "    article_index: target article\n",
    "    k = get the top k articles\n",
    "    include_negative: if abs values should be used\n",
    "    return top articles for given index\n",
    "    \"\"\"\n",
    "\n",
    "    # Calculate pairwise cosine similarity with sklearn library\n",
    "    similarity_scores = cosine_similarity(embeddings)\n",
    "    \n",
    "    # get similarity scores for the given article index\n",
    "    article_scores = similarity_scores[article_index]\n",
    "        \n",
    "    # get negative similarity or positive ones\n",
    "    # argsort returns the indices of the sorted array\n",
    "    if negativSimilartiy:\n",
    "        # [:k] to return only the top k article indices \n",
    "        sorted_indices = np.argsort(article_scores)[:k] \n",
    "    else:\n",
    "        # [::-1] to order in decending order\n",
    "        # 1:k+1 to exclude the target-article itself (similartiy = 1)\n",
    "        sorted_indices = np.argsort(article_scores)[::-1][1:k+1] \n",
    "    \n",
    "    # Get the top k (anti-) similar articles\n",
    "    top_articles = [(index, article_scores[index]) for index in sorted_indices if index != article_index]\n",
    " \n",
    "    return top_articles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e115059-8d56-4296-a34a-cc814e8f4792",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "086327d3-9511-4b91-a8e4-23b897312070",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TEST Function with and without abs-values\n",
    "\n",
    "target_article = 10\n",
    "k = 5\n",
    "\n",
    "# Get top similar articles for article at index 0\n",
    "top_articles = get_topk_similar_articles(query_result, target_article, k, negativSimilartiy=False)\n",
    "\n",
    "print(f\"Top {k} Articles for Index: {target_article}:\")\n",
    "# Print the top similar articles\n",
    "for article_index, similarity in top_articles:\n",
    "    print(f\"Article Index: {article_index}, Similarity Score: {similarity}\")\n",
    "\n",
    "    \n",
    "    \n",
    "# Get top anti-similar articles for article at index 0\n",
    "top_articles = get_topk_similar_articles(query_result, target_article, k, negativSimilartiy=True)\n",
    "\n",
    "print(f\"\\nTop {k} anti-similar Articles for Index: {target_article}:\")\n",
    "# Print the top similar articles\n",
    "for article_index, similarity in top_articles:\n",
    "    print(f\"Article Index: {article_index}, Similarity Score: {similarity}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd9840f8-8e36-4f14-8ee5-aee0747383a5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cf9c4d6-9d89-4fd5-a12e-f036df2e7e28",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25e7ec82-054e-42ab-b48f-ff9903d746b0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76296b47-6b55-451c-b1f2-d14b201495de",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d67415df-d170-4ba5-9e24-cff5872a06f9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5d8f1d2-acf8-4173-bcf9-5ff7dc0a6dab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Have to get the topic of each article from the newdf and make a heatmap to check if the recommendation is from the same topic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb81df56-6a74-4ec9-abb6-92565b013b1a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "k = 1\n",
    "vals = {}\n",
    "\n",
    "for target_article in range(500):\n",
    "    top_articles = get_topk_similar_articles(query_result, target_article, k, negativSimilartiy=False)\n",
    "    #print(top_articles[0][0])\n",
    "    ressort = newdf.loc[target_article, \"ressort\"]\n",
    "    date = newdf.loc[target_article, \"date\"]\n",
    "    ressort2 = newdf.loc[top_articles[0][0], \"ressort\"]\n",
    "    date2 = newdf.loc[top_articles[0][0], \"date\"]\n",
    "\n",
    "    \n",
    "    vals[target_article] = {\n",
    "        'target_article': target_article,\n",
    "        'top_article': top_articles[0][0],\n",
    "        'ressort': ressort,\n",
    "        'ressort2': ressort2,\n",
    "        'date': date,\n",
    "        'date2': date2\n",
    "    }   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "116b9b81-269a-4bb7-b934-90107da90d96",
   "metadata": {},
   "outputs": [],
   "source": [
    "dfheatmap = pd.DataFrame.from_dict(vals, orient='index')\n",
    "\n",
    "heatmap_data = pd.crosstab(dfheatmap['ressort'], dfheatmap['ressort2'])\n",
    "\n",
    "# Create the heatmap using seaborn\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(heatmap_data, cmap='YlGnBu', annot=True, fmt=\"d\", cbar=True)\n",
    "plt.xlabel('Ressort 2')\n",
    "plt.ylabel('Ressort')\n",
    "plt.title('Heatmap of Ressort by Ressort 2')\n",
    "\n",
    "# Display the heatmap\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67436148-525d-473f-89ab-65db447d67c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#dfheatmap['date'] = pd.to_datetime(dfheatmap['date'])\n",
    "#dfheatmap['date2'] = pd.to_datetime(dfheatmap['date2'])\n",
    "\n",
    "# Calculate the difference in days\n",
    "\n",
    "#if dfheatmap['date']>dfheatmap['date2']:\n",
    "    \n",
    "    #dfheatmap['date_diff'] = -1*(dfheatmap['date'] - dfheatmap['date2']).dt.days\n",
    "    \n",
    "# Plot the histogram\n",
    "#plt.figure(figsize=(8, 6))\n",
    "#sns.histplot(data=dfheatmap, x='date_diff', bins=30, kde=True)\n",
    "#plt.title('Histogram of Date Difference')\n",
    "#plt.xlabel('Days Difference')\n",
    "#plt.ylabel('Frequency')\n",
    "#plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5af01c94-661f-4388-a21a-33e5209d6bc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "dfheatmap['date'] = pd.to_datetime(dfheatmap['date'])\n",
    "dfheatmap['date2'] = pd.to_datetime(dfheatmap['date2'])\n",
    "\n",
    "# Calculate the difference in days\n",
    "\n",
    "dfheatmap['date'] = pd.to_datetime(dfheatmap['date'])\n",
    "dfheatmap['date2'] = pd.to_datetime(dfheatmap['date2'])\n",
    "\n",
    "# Calculate the difference in days\n",
    "dfheatmap['date_diff'] = (dfheatmap['date'] - dfheatmap['date2']).dt.days\n",
    "\n",
    "# Display the DataFrame\n",
    "print(dfheatmap.sort_values(by='date_diff'))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f8f61e0-d4a2-43e7-bad1-892b6ce66e22",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set_style(\"whitegrid\")\n",
    "\n",
    "# Create the plot\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.histplot(data=dfheatmap, x='date_diff', bins=30, color='skyblue', kde=True, linewidth=0)\n",
    "\n",
    "# Add labels and title\n",
    "plt.title('Histogram of Date Differences', fontsize=16)\n",
    "plt.xlabel('Days Difference', fontsize=14)\n",
    "plt.ylabel('Frequency', fontsize=14)\n",
    "\n",
    "# Remove the top and right spines from plot for a cleaner look\n",
    "sns.despine()\n",
    "\n",
    "# Show the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45db1e55-6e15-4975-b343-26c026b68f35",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e22fb9c2-3615-4ec5-bb09-f1fc1b1809ca",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3d617a1-1030-4a47-97b0-64ce2c7b73c3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9604ea0e-d0cb-4b6d-ad75-736354091760",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db758f9d-fde3-4016-b2b1-9d9ccabd1e5b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a686616b-05fb-43b8-bfe9-e92914c4e6f9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "648981c3-b022-4793-9961-ad37bef7ea4d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "108cfc11-4f14-41cf-a693-0229c7f9899b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1dba554d-fc81-417b-86a2-444ce9c6155f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.embeddings import FakeEmbeddings\n",
    "fakeemb = FakeEmbeddings(size=768)\n",
    "embd = fakeemb.embed_documents(df_picked.Top_Words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c58669ca-8656-4e84-a63a-90d50eb42363",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TEST Function with and without abs-values\n",
    "\n",
    "target_article = 10\n",
    "k = 5\n",
    "\n",
    "# Get top similar articles for article at index 0\n",
    "top_articles = get_topk_similar_articles(embd, target_article, k, negativSimilartiy=False)\n",
    "\n",
    "print(f\"Top {k} Articles for Index: {target_article}:\")\n",
    "# Print the top similar articles\n",
    "for article_index, similarity in top_articles:\n",
    "    print(f\"Article Index: {article_index}, Similarity Score: {similarity}\")\n",
    "\n",
    "    \n",
    "    \n",
    "# Get top anti-similar articles for article at index 0\n",
    "top_articles = get_topk_similar_articles(embd, target_article, k, negativSimilartiy=True)\n",
    "\n",
    "print(f\"\\nTop {k} anti-similar Articles for Index: {target_article}:\")\n",
    "# Print the top similar articles\n",
    "for article_index, similarity in top_articles:\n",
    "    print(f\"Article Index: {article_index}, Similarity Score: {similarity}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60759c49-3cf0-4292-a13f-3870a2d3cdcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_name = \"huggingface.pkl\"\n",
    "\n",
    "open_file = open(file_name, \"wb\")\n",
    "pickle.dump(query_result, open_file) \n",
    "open_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1b50459-32a8-4091-9158-405dde165789",
   "metadata": {},
   "outputs": [],
   "source": [
    "open_file = open(file_name, \"rb\")\n",
    "loaded_list = pickle.load(open_file)\n",
    "open_file.close()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5756fc27-5852-4fb2-bf43-4c8f65bc68b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.embeddings import OpenAIEmbeddings\n",
    "openai = OpenAIEmbeddings(openai_api_key=\"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\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f4b0b6e-dc11-49bc-95f0-780d30d02c8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "openai = openai.embed_documents(df_picked.Top_Words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f93bdd69-0e31-4215-8ffc-522bd09d526c",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_name = \"openaiembed.pkl\"\n",
    "\n",
    "open_file = open(file_name, \"wb\")\n",
    "pickle.dump(openai, open_file) # !!!!!!!!!!!!!!!!!!!!!!!!!should not it be openai instead query_results????\n",
    "open_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08711b7b-f0c1-4165-a8f5-621062a96fc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TEST Function with and without abs-values\n",
    "\n",
    "target_article = 10\n",
    "k = 5\n",
    "\n",
    "# Get top similar articles for article at index 0\n",
    "top_articles =  (openai, target_article, k, negativSimilartiy=False)\n",
    "\n",
    "print(f\"Top {k} Articles for Index: {target_article}:\")\n",
    "# Print the top similar articles\n",
    "for article_index, similarity in top_articles:\n",
    "    print(f\"Article Index: {article_index}, Similarity Score: {similarity}\")\n",
    "\n",
    "    \n",
    "    \n",
    "# Get top anti-similar articles for article at index 0\n",
    "top_articles = get_topk_similar_articles(openai, target_article, k, negativSimilartiy=True)\n",
    "\n",
    "print(f\"\\nTop {k} anti-similar Articles for Index: {target_article}:\")\n",
    "# Print the top similar articles\n",
    "for article_index, similarity in top_articles:\n",
    "    print(f\"Article Index: {article_index}, Similarity Score: {similarity}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b652d77-41c5-4871-8c82-1068aecce9a2",
   "metadata": {},
   "source": [
    "# Start From Here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65242fac-f654-41af-aa3c-3578016efc8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cleaned = pd.read_csv(\"cleaned.csv\", index_col=False).drop(\"Unnamed: 0\", axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d094f96e-d6a4-4310-b020-4f3eee0e17bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cleaned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d3ae8b2-e3d3-4bfc-800f-8cc57bda0023",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_picked = pd.read_csv(\"top_80.csv\")\n",
    "df_picked.drop(\"Unnamed: 0\", axis=1, inplace=True)\n",
    "df_picked"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "845e3f74-59e4-4998-bb3e-209b298585ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "open_file = open(\"huggingface_embeddings.pkl\", \"rb\")\n",
    "hugging = pickle.load(open_file)\n",
    "open_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a6475f8-2f0f-4053-b3b8-5f9f1d019fd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "open_file = open(\"openaiembed.pkl\", \"rb\")\n",
    "openaiemb = pickle.load(open_file)\n",
    "open_file.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "044e4af5-6674-45b0-95ad-35a78ea3d3d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "def get_topk_similar_articles(embeddings, article_index, k, negativSimilartiy=False):\n",
    "    \"\"\"\n",
    "    Get the top k news articles with highest similarity for a given index\n",
    "    \n",
    "    embeddings: embeddings matrix\n",
    "    article_index: target article\n",
    "    k = get the top k articles\n",
    "    include_negative: if abs values should be used\n",
    "    return top articles for given index\n",
    "    \"\"\"\n",
    "\n",
    "    # Calculate pairwise cosine similarity with sklearn library\n",
    "    similarity_scores = cosine_similarity(embeddings)\n",
    "    \n",
    "    # get similarity scores for the given article index\n",
    "    article_scores = similarity_scores[article_index]\n",
    "        \n",
    "    # get negative similarity or positive ones\n",
    "    # argsort returns the indices of the sorted array\n",
    "    if negativSimilartiy:\n",
    "        # [:k] to return only the top k article indices \n",
    "        sorted_indices = np.argsort(article_scores)[:k] \n",
    "    else:\n",
    "        # [::-1] to order in decending order\n",
    "        # 1:k+1 to exclude the target-article itself (similartiy = 1)\n",
    "        sorted_indices = np.argsort(article_scores)[::-1][1:k+1] \n",
    "    \n",
    "    # Get the top k (anti-) similar articles\n",
    "    top_articles = [(index, article_scores[index]) for index in sorted_indices if index != article_index]\n",
    " \n",
    "    return top_articles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a65623a6-ad20-40c2-bf98-c7f9493761f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TEST Function with and without abs-values\n",
    "\n",
    "target_article = 10\n",
    "k = 5\n",
    "\n",
    "# Get top similar articles for article at index 0\n",
    "top_articles = get_topk_similar_articles(openaiemb, target_article, k, negativSimilartiy=False)\n",
    "\n",
    "print(f\"Top {k} Articles for Index: {target_article}:\")\n",
    "# Print the top similar articles\n",
    "for article_index, similarity in top_articles:\n",
    "    print(f\"Article Index: {article_index}, Similarity Score: {similarity}\")\n",
    "\n",
    "    \n",
    "    \n",
    "# Get top anti-similar articles for article at index 0\n",
    "top_articles = get_topk_similar_articles(openaiemb, target_article, k, negativSimilartiy=True)\n",
    "\n",
    "print(f\"\\nTop {k} anti-similar Articles for Index: {target_article}:\")\n",
    "# Print the top similar articles\n",
    "for article_index, similarity in top_articles:\n",
    "    print(f\"Article Index: {article_index}, Similarity Score: {similarity}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b057335d-3bc1-4c37-8d02-3bbd6d749ae7",
   "metadata": {},
   "outputs": [],
   "source": [
    "target_article = 10\n",
    "k = 5\n",
    "\n",
    "# Get top similar articles for article at index 0\n",
    "top_articles = get_topk_similar_articles(hugging, target_article, k, negativSimilartiy=False)\n",
    "\n",
    "print(f\"Top {k} Articles for Index: {target_article}:\")\n",
    "# Print the top similar articles\n",
    "for article_index, similarity in top_articles:\n",
    "    print(f\"Article Index: {article_index}, Similarity Score: {similarity}\")\n",
    "\n",
    "    \n",
    "    \n",
    "# Get top anti-similar articles for article at index 0\n",
    "top_articles = get_topk_similar_articles(hugging, target_article, k, negativSimilartiy=True)\n",
    "\n",
    "print(f\"\\nTop {k} anti-similar Articles for Index: {target_article}:\")\n",
    "# Print the top similar articles\n",
    "for article_index, similarity in top_articles:\n",
    "    print(f\"Article Index: {article_index}, Similarity Score: {similarity}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73f12c2a-b23c-4ab0-aba2-8636201bdeb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.embeddings import FakeEmbeddings\n",
    "fakeemb = FakeEmbeddings(size=768)\n",
    "embd = fakeemb.embed_documents(df_picked.Top_Words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2da6b70-e863-4a72-bc1c-327856d5b41e",
   "metadata": {},
   "outputs": [],
   "source": [
    "target_article = 10\n",
    "k = 5\n",
    "\n",
    "# Get top similar articles for article at index 0\n",
    "top_articles = get_topk_similar_articles(embd, target_article, k, negativSimilartiy=False)\n",
    "\n",
    "print(f\"Top {k} Articles for Index: {target_article}:\")\n",
    "# Print the top similar articles\n",
    "for article_index, similarity in top_articles:\n",
    "    print(f\"Article Index: {article_index}, Similarity Score: {similarity}\")\n",
    "\n",
    "    \n",
    "    \n",
    "# Get top anti-similar articles for article at index 0\n",
    "top_articles = get_topk_similar_articles(embd, target_article, k, negativSimilartiy=True)\n",
    "\n",
    "print(f\"\\nTop {k} anti-similar Articles for Index: {target_article}:\")\n",
    "# Print the top similar articles\n",
    "for article_index, similarity in top_articles:\n",
    "    print(f\"Article Index: {article_index}, Similarity Score: {similarity}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01343884-7644-444e-931d-6bce49ffd951",
   "metadata": {},
   "outputs": [],
   "source": [
    "withsim = df_cleaned[~df_cleaned.Similarity.isna()]\n",
    "withsim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66d174c9-1024-4c7c-930c-40d3b1f20306",
   "metadata": {},
   "outputs": [],
   "source": [
    "withsim.article_id.nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f8efc67-e720-401e-b143-75e78d0b9d4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_picked"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "864d3e97-9bdb-4835-aef9-8d95ec4764b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#withsim.join(df_picked, on=\"article_id\", how=\"inner\"joined=)\n",
    "joined = withsim.merge(right=df_picked, how=\"left\", on=\"article_id\")\n",
    "joined"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bbaa0c3-0024-429e-927e-f22391c5aa8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "joined['class'] = pd.cut(joined['Similarity'], bins=[-1, -0.5, 0.5, 1], labels=[0, 1, 2], right=False)\n",
    "joined\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63586f8e-fc7b-4b33-89ed-60aefe0631f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "joined.article_id.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98edfae0-c9a3-409f-b4dc-668cb3c050d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "fakeemb = FakeEmbeddings(size=100)\n",
    "mappings = {}\n",
    "for i in joined.article_id.unique():\n",
    "    joined[joined.article_id == i]\n",
    "    embd = fakeemb.embed_documents(joined[joined.article_id == i].iloc[0].Top_Words)\n",
    "    mappings[i] = embd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "884b6381-e570-4b48-a541-b021f58b13e3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "caed43e0-4989-497c-abd5-febfc1193e64",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2864e91d-a7ce-4c5d-8735-a9b2250657e4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f24cef32-167c-4a8d-ab8e-2827ac2e7a9e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "313687d4-5e95-4738-afbe-20a746b0bb48",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1d388f7-906d-42d5-8431-94a3916db80b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
