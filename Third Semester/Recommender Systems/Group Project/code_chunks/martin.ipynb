{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3f0e3799-1472-4ccb-8e3d-0313621412c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting langchain\n",
      "  Downloading langchain-0.0.229-py3-none-any.whl (1.3 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m22.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: PyYAML>=5.4.1 in /opt/conda/lib/python3.10/site-packages (from langchain) (6.0)\n",
      "Collecting dataclasses-json<0.6.0,>=0.5.7\n",
      "  Using cached dataclasses_json-0.5.9-py3-none-any.whl (26 kB)\n",
      "Collecting numexpr<3.0.0,>=2.8.4\n",
      "  Using cached numexpr-2.8.4-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (381 kB)\n",
      "Requirement already satisfied: numpy<2,>=1 in /opt/conda/lib/python3.10/site-packages (from langchain) (1.23.5)\n",
      "Collecting openapi-schema-pydantic<2.0,>=1.2\n",
      "  Using cached openapi_schema_pydantic-1.2.4-py3-none-any.whl (90 kB)\n",
      "Requirement already satisfied: requests<3,>=2 in /opt/conda/lib/python3.10/site-packages (from langchain) (2.28.2)\n",
      "Requirement already satisfied: async-timeout<5.0.0,>=4.0.0 in /opt/conda/lib/python3.10/site-packages (from langchain) (4.0.2)\n",
      "Requirement already satisfied: tenacity<9.0.0,>=8.1.0 in /opt/conda/lib/python3.10/site-packages (from langchain) (8.2.2)\n",
      "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /opt/conda/lib/python3.10/site-packages (from langchain) (2.0.1)\n",
      "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /opt/conda/lib/python3.10/site-packages (from langchain) (3.8.4)\n",
      "Collecting pydantic<2,>=1\n",
      "  Using cached pydantic-1.10.11-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.1 MB)\n",
      "Collecting langchainplus-sdk<0.0.21,>=0.0.20\n",
      "  Using cached langchainplus_sdk-0.0.20-py3-none-any.whl (25 kB)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.8.2)\n",
      "Requirement already satisfied: charset-normalizer<4.0,>=2.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (2.1.1)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /opt/conda/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (6.0.4)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (22.2.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /opt/conda/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.3.3)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /opt/conda/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.3.1)\n",
      "Collecting marshmallow<4.0.0,>=3.3.0\n",
      "  Using cached marshmallow-3.19.0-py3-none-any.whl (49 kB)\n",
      "Collecting marshmallow-enum<2.0.0,>=1.5.1\n",
      "  Using cached marshmallow_enum-1.5.1-py2.py3-none-any.whl (4.2 kB)\n",
      "Collecting typing-inspect>=0.4.0\n",
      "  Using cached typing_inspect-0.9.0-py3-none-any.whl (8.8 kB)\n",
      "Requirement already satisfied: typing-extensions>=4.2.0 in /opt/conda/lib/python3.10/site-packages (from pydantic<2,>=1->langchain) (4.4.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests<3,>=2->langchain) (3.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests<3,>=2->langchain) (2022.12.7)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests<3,>=2->langchain) (1.26.14)\n",
      "Requirement already satisfied: greenlet!=0.4.17 in /opt/conda/lib/python3.10/site-packages (from SQLAlchemy<3,>=1.4->langchain) (2.0.2)\n",
      "Requirement already satisfied: packaging>=17.0 in /opt/conda/lib/python3.10/site-packages (from marshmallow<4.0.0,>=3.3.0->dataclasses-json<0.6.0,>=0.5.7->langchain) (23.0)\n",
      "Requirement already satisfied: mypy-extensions>=0.3.0 in /opt/conda/lib/python3.10/site-packages (from typing-inspect>=0.4.0->dataclasses-json<0.6.0,>=0.5.7->langchain) (1.0.0)\n",
      "Installing collected packages: typing-inspect, pydantic, numexpr, marshmallow, openapi-schema-pydantic, marshmallow-enum, langchainplus-sdk, dataclasses-json, langchain\n",
      "  Attempting uninstall: numexpr\n",
      "    Found existing installation: numexpr 2.8.3\n",
      "    Uninstalling numexpr-2.8.3:\n",
      "      Successfully uninstalled numexpr-2.8.3\n",
      "Successfully installed dataclasses-json-0.5.9 langchain-0.0.229 langchainplus-sdk-0.0.20 marshmallow-3.19.0 marshmallow-enum-1.5.1 numexpr-2.8.4 openapi-schema-pydantic-1.2.4 pydantic-1.10.11 typing-inspect-0.9.0\n"
     ]
    }
   ],
   "source": [
    "!pip install langchain\n",
    "\n",
    "import os\n",
    "import pandas as pd\n",
    "from langchain.embeddings import FakeEmbeddings\n",
    "from scipy import spatial\n",
    "\n",
    "import numpy as np\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "import pickle\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "70b4d986-7516-46f5-afeb-8732d68bc299",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_topk_similar_articles(embeddings, article_index, k, anti_similarity=False):\n",
    "    \"\"\"\n",
    "    Get the top k news articles with highest similarity for a given index\n",
    "    \n",
    "    embeddings: embeddings matrix\n",
    "    article_index: target article\n",
    "    k = get the top k articles\n",
    "    anti_similarity: if anti similar values should be used\n",
    "    return top articles for given index\n",
    "    \"\"\"\n",
    "\n",
    "    # Calculate pairwise cosine similarity with sklearn library\n",
    "    similarity_scores = cosine_similarity(embeddings)\n",
    "    \n",
    "    # get similarity scores for the given article index\n",
    "    article_scores = similarity_scores[article_index]\n",
    "        \n",
    "    # get anti_similarity or positive ones\n",
    "    # argsort returns the indices of the sorted array\n",
    "    if anti_similarity:\n",
    "        # [:k] to return only the top k article indices \n",
    "        sorted_indices = np.argsort(article_scores)[:k] \n",
    "    else:\n",
    "        # [::-1] to order in decending order\n",
    "        # 1:k+1 to exclude the target-article itself (similartiy = 1)\n",
    "        sorted_indices = np.argsort(article_scores)[::-1][1:k+1] \n",
    "    \n",
    "    # Get the top k (anti-) similar articles\n",
    "    top_articles = [(index, article_scores[index]) for index in sorted_indices if index != article_index]\n",
    " \n",
    "    return top_articles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fafc49b9-1338-4f69-885f-37baff73788c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculated_cosine_similarity(embeddings):\n",
    "    ''' Calculate pairwise similarities and return them '''\n",
    "    # Calculate pairwise cosine similarity with sklearn library\n",
    "    similarity_scores = cosine_similarity(embeddings)\n",
    "    return similarity_scores\n",
    "\n",
    "def get_topk_similar_articles_precalculated(similarity_scores, article_index, k, anti_similarity=False):\n",
    "    \"\"\"\n",
    "    Get the top k similar articles for a given article_index from the precomputed cos-similarties\n",
    "    \"\"\"\n",
    "    \n",
    "    # get similarity scores for the given article index\n",
    "    article_scores = similarity_scores[article_index]\n",
    "        \n",
    "    # get anti_similarity or positive ones\n",
    "    # argsort returns the indices of the sorted array\n",
    "    if anti_similarity:\n",
    "        # [:k] to return only the top k article indices \n",
    "        sorted_indices = np.argsort(article_scores)[:k] \n",
    "    else:\n",
    "        # [::-1] to order in decending order\n",
    "        # 1:k+1 to exclude the target-article itself (similartiy = 1)\n",
    "        sorted_indices = np.argsort(article_scores)[::-1][1:k+1] \n",
    "    \n",
    "    # Get the top k (anti-) similar articles\n",
    "    top_articles = [(index, article_scores[index]) for index in sorted_indices if index != article_index]\n",
    " \n",
    "    return top_articles\n",
    "\n",
    "def get_similarty_for_two_articles(similarity_scores, article_index, compare_index):\n",
    "    article_scores = similarity_scores[article_index]\n",
    "    return article_scores[compare_index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30514c02-0609-42da-ab54-30b1a1b4bead",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3bd0f071-26c7-492f-ae65-a66e9cab4c66",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5916\n",
      "2955\n"
     ]
    }
   ],
   "source": [
    "cleaned_articles = pd.read_csv(\"../data/cleaned_articles.csv\", index_col=False).drop(\"Unnamed: 0\", axis = 1)\n",
    "\n",
    "study_articles = pd.read_csv(\"../data/rating_similarity.csv\", index_col=False).drop(\"Unnamed: 0\", axis = 1)\n",
    "\n",
    "print(len(cleaned_articles))\n",
    "print(len(study_articles))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "056c842c-8062-46ae-8ec1-cd3852067ee0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>article_id</th>\n",
       "      <th>title</th>\n",
       "      <th>paragraphs</th>\n",
       "      <th>ressort</th>\n",
       "      <th>authors</th>\n",
       "      <th>date</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>FALTER_20151223BA00BC1175</td>\n",
       "      <td>DIE BLACK STREET BOYS</td>\n",
       "      <td>['Harald Mahrer: Der Nachdenkliche', 'Der Älte...</td>\n",
       "      <td>Politik</td>\n",
       "      <td>josef redl, barbara tóth</td>\n",
       "      <td>2015-12-23 00:00:00+00:00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                   article_id                  title  \\\n",
       "14  FALTER_20151223BA00BC1175  DIE BLACK STREET BOYS   \n",
       "\n",
       "                                           paragraphs  ressort  \\\n",
       "14  ['Harald Mahrer: Der Nachdenkliche', 'Der Älte...  Politik   \n",
       "\n",
       "                     authors                       date  \n",
       "14  josef redl, barbara tóth  2015-12-23 00:00:00+00:00  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cleaned_articles.loc[cleaned_articles['article_id'] == \"FALTER_20151223BA00BC1175\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ebc0567a-6013-4b98-b88d-3a6ecabdc03f",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Get article_id's of compared articles for a specific article \n",
    "'''\n",
    "def get_articles_for_base_article(base_article):\n",
    "    list = study_articles.loc[study_articles['article_id'] == base_article]\n",
    "    compared_article_ids = list.compared_article.values\n",
    "    return compared_article_ids\n",
    "\n",
    "\n",
    "# --------------------------------------------------------\n",
    "def get_index_for_articleid(article_id):\n",
    "    return cleaned_articles.loc[cleaned_articles['article_id'] == article_id].index.values[0]\n",
    "\n",
    "\n",
    "\n",
    "# --------------------------------------------------------\n",
    "def intersection(list_a, list_b):\n",
    "    return [ e for e in list_a if e in list_b ]\n",
    "\n",
    "\n",
    "\n",
    "# --------------------------------------------------------\n",
    "# get list of based-articles\n",
    "def get_base_articles():\n",
    "        list = study_articles['article_id']\n",
    "        return set(list)\n",
    "#for item in base_articles:\n",
    "#    falter_articles_ids = get_articles_for_base_article(item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8962b984-9c9d-4d68-987a-69554a616cbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def intersection_calculation(embeddings_file, embeddings_file2=0, use_second_embeddings=True, num_articles=15):\n",
    "    ''' Intersection calculation for embeddings with the labeled dataset '''\n",
    "    # -> calculate embeddings\n",
    "    # -> function to get index from article_id # get_index_for_articleid(article_id)\n",
    "    # -> base_articles # get_base_articles()\n",
    "    # -> compared_articles for a article()\n",
    "    # -> similarities from embeddings\n",
    "    # -> compare top 15 articles\n",
    "    \n",
    "    open_file = open(embeddings_file, \"rb\")\n",
    "    embeddings = pickle.load(open_file)\n",
    "    open_file.close()\n",
    "    similarity_scores = calculated_cosine_similarity(embeddings)\n",
    "    max_intersec = 0\n",
    "    num_same_articles = 0\n",
    "    \n",
    "    if use_second_embeddings:\n",
    "        base_articles = get_base_articles() # article_id of the articles that got compared\n",
    "\n",
    "        for article in base_articles:\n",
    "            labeled_dataset_articles = get_articles_for_base_article(article) # article ids for a base article\n",
    "            recommended_articles_with_sim = get_topk_similar_articles_precalculated(similarity_scores, get_index_for_articleid(article), num_articles, False)\n",
    "\n",
    "            indices_calculated = [item[0] for item in recommended_articles_with_sim]\n",
    "            indices_labled_dataset = [get_index_for_articleid(item) for item in labeled_dataset_articles]\n",
    "            intersec = intersection(indices_labled_dataset, indices_calculated)\n",
    "            num_same_articles += len(intersec)\n",
    "        \n",
    "        max_intersec = (len(base_articles)*15)\n",
    "\n",
    "    else:\n",
    "        open_file = open(embeddings_file2, \"rb\")\n",
    "        embeddings2 = pickle.load(open_file)\n",
    "        open_file.close()\n",
    "        similarity_scores2 = calculated_cosine_similarity(embeddings2)\n",
    "        \n",
    "        for i in range(5916):\n",
    "            recommended_articles_with_sim = get_topk_similar_articles_precalculated(similarity_scores, i, num_articles, False)\n",
    "            recommended_articles_with_sim2 = get_topk_similar_articles_precalculated(similarity_scores2, i, num_articles, False)\n",
    "            indices_calculated = [item[0] for item in recommended_articles_with_sim]\n",
    "            indices_calculated2 = [item[0] for item in recommended_articles_with_sim2]\n",
    "            intersec = intersection(indices_calculated, indices_calculated2)\n",
    "            num_same_articles += len(intersec)\n",
    "            \n",
    "        max_intersec = (5916*15)\n",
    "\n",
    "    print('Number of articles that are in in the intersection: ', num_same_articles)\n",
    "    coverage = \"Intersection in percentage: {:.2f}%\".format(100/max_intersec * num_same_articles)\n",
    "    print(coverage)\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9218a1ba-b881-42c0-98c7-ac826df3ed3a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OpenAI 80 Words:\n",
      "Number of articles that are in in the intersection:  467\n",
      "Intersection in percentage: 15.80%\n",
      "\n",
      "\n",
      "HuggingFace 80 Words:\n",
      "Number of articles that are in in the intersection:  251\n",
      "Intersection in percentage: 8.49%\n",
      "\n",
      "\n",
      "OpenAI 250 Words:\n",
      "Number of articles that are in in the intersection:  619\n",
      "Intersection in percentage: 20.95%\n",
      "\n",
      "\n",
      "HuggingFace 250 Words:\n",
      "Number of articles that are in in the intersection:  404\n",
      "Intersection in percentage: 13.67%\n",
      "\n",
      "\n",
      "OpenAI 80 words vs OpenAI 250 words with 15 articles:\n",
      "Number of articles that are in in the intersection:  49964\n",
      "Intersection in percentage: 56.30%\n",
      "\n",
      "\n",
      "OpenAI 80 words vs OpenAI 250 words with 25 articles:\n",
      "Number of articles that are in in the intersection:  85468\n",
      "Intersection in percentage: 96.31%\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#197 Base articles => 197 * 15 = 2955 articles in theory \n",
    "\n",
    "print('OpenAI 80 Words:')\n",
    "intersection_calculation(\"../data/openaiembed.pkl\")\n",
    "print('HuggingFace 80 Words:')\n",
    "intersection_calculation(\"../data/huggingface_embeddings.pkl\")\n",
    "print('OpenAI 250 Words:')\n",
    "intersection_calculation(\"../data/openaiembed_top250.pkl\")\n",
    "print('HuggingFace 250 Words:')\n",
    "intersection_calculation(\"../data/huggingface_top250_embedding.pkl\")\n",
    "\n",
    "print('OpenAI 80 words vs OpenAI 250 words with 15 articles:')\n",
    "intersection_calculation(\"../data/openaiembed_top250.pkl\", \"../data/openaiembed.pkl\", False)\n",
    "\n",
    "print('OpenAI 80 words vs OpenAI 250 words with 25 articles:')\n",
    "intersection_calculation(\"../data/openaiembed_top250.pkl\", \"../data/openaiembed.pkl\", False, 25)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b4a535af-a9b8-4208-a0b2-6057e15b4f07",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def get_min_max_similarties(filename):\n",
    "    open_file = open(filename, \"rb\")\n",
    "    embeddings = pickle.load(open_file)\n",
    "    open_file.close()\n",
    "\n",
    "    similarity_scores = calculated_cosine_similarity(embeddings)\n",
    "\n",
    "    # def get_topk_similar_articles_precalculated(similarity_scores, article_index, k, negativSimilartiy=False):\n",
    "\n",
    "\n",
    "    min_sim = 1\n",
    "    max_sim = 0\n",
    "    for i in range(5916):\n",
    "        list = get_topk_similar_articles_precalculated(similarity_scores, i, 1, True)\n",
    "        sim = list[0][1]\n",
    "        min_sim = min(min_sim, sim)\n",
    "        list = get_topk_similar_articles_precalculated(similarity_scores, i, 2, False)\n",
    "        sim = list[0][1]\n",
    "        if sim < 1:\n",
    "            max_sim = max(max_sim, sim)    \n",
    "\n",
    "    print('Minimal cos-sim for all articles: ', min_sim)\n",
    "    print('Maximal cos-sim for all articles: ', max_sim)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "449b4548-417a-49de-97d2-85b158e5f23f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OpenAI Top80 Embeddings: \n",
      "Minimal cos-sim for all articles:  0.7296835164144966\n",
      "Maximal cos-sim for all articles:  0.9974947860618575\n",
      "\n",
      "OpenAI Top250 Embeddings: \n",
      "Minimal cos-sim for all articles:  0.6979026654793344\n",
      "Maximal cos-sim for all articles:  0.9999999999999998\n",
      "\n",
      "HuggingFace Top80 Embeddings: \n",
      "Minimal cos-sim for all articles:  -0.11489994902211896\n",
      "Maximal cos-sim for all articles:  0.9975477057357242\n",
      "\n",
      "HuggingFace Top250 Embeddings: \n",
      "Minimal cos-sim for all articles:  0.26555460047357027\n",
      "Maximal cos-sim for all articles:  0.999478314915158\n"
     ]
    }
   ],
   "source": [
    "print(\"OpenAI Top80 Embeddings: \")\n",
    "get_min_max_similarties(\"../data/openaiembed_top250.pkl\")\n",
    "\n",
    "print(\"\\nOpenAI Top250 Embeddings: \")\n",
    "get_min_max_similarties(\"../data/openaiembed.pkl\")\n",
    "\n",
    "print(\"\\nHuggingFace Top80 Embeddings: \")\n",
    "get_min_max_similarties(\"../data/huggingface_embeddings.pkl\")\n",
    "\n",
    "print(\"\\nHuggingFace Top250 Embeddings: \")\n",
    "get_min_max_similarties(\"../data/huggingface_top250_embedding.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a197f789-59ec-4bf3-bac4-aa7444b3a07d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting openai\n",
      "  Using cached openai-0.27.8-py3-none-any.whl (73 kB)\n",
      "Requirement already satisfied: requests>=2.20 in /opt/conda/lib/python3.10/site-packages (from openai) (2.28.2)\n",
      "Requirement already satisfied: tqdm in /opt/conda/lib/python3.10/site-packages (from openai) (4.64.1)\n",
      "Requirement already satisfied: aiohttp in /opt/conda/lib/python3.10/site-packages (from openai) (3.8.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests>=2.20->openai) (3.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests>=2.20->openai) (2022.12.7)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests>=2.20->openai) (1.26.14)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests>=2.20->openai) (2.1.1)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /opt/conda/lib/python3.10/site-packages (from aiohttp->openai) (1.3.3)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /opt/conda/lib/python3.10/site-packages (from aiohttp->openai) (4.0.2)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /opt/conda/lib/python3.10/site-packages (from aiohttp->openai) (6.0.4)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->openai) (22.2.0)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /opt/conda/lib/python3.10/site-packages (from aiohttp->openai) (1.3.1)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->openai) (1.8.2)\n",
      "Installing collected packages: openai\n",
      "Successfully installed openai-0.27.8\n"
     ]
    }
   ],
   "source": [
    "!pip install openai\n",
    "\n",
    "import openai\n",
    "import pandas as pd\n",
    "openai.api_key = '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'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "06cdc978-fe6e-456a-a81a-12f9e83a8b74",
   "metadata": {},
   "outputs": [],
   "source": [
    "eng = pd.read_csv(\"../data/articles_2015_translated.csv\", index_col=False)\n",
    "eng = eng[[\"article_id\", \"paragraphs\", \"title\", \"title_en\", \"paragraphs_en\"]]\n",
    "\n",
    "ger = pd.read_csv(\"../data/articles_2015.csv\", index_col=False)\n",
    "ger = ger[[\"article_id\", \"paragraphs\", \"title\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7fb0dd3a-db09-488a-9d95-41f821b8c4fb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>article_id</th>\n",
       "      <th>title</th>\n",
       "      <th>paragraphs</th>\n",
       "      <th>ressort</th>\n",
       "      <th>authors</th>\n",
       "      <th>date</th>\n",
       "      <th>text_cleaned</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>5913</th>\n",
       "      <td>FALTER_201501144F8F9DDBEE</td>\n",
       "      <td>Mit einem Flüchtling in der WG: Neue Initiativ...</td>\n",
       "      <td>['Warum müssen Menschen, die vor Krieg und Ele...</td>\n",
       "      <td>Politik</td>\n",
       "      <td>benedikt narodoslawsky</td>\n",
       "      <td>2015-01-14 00:00:00+00:00</td>\n",
       "      <td>warum mssen menschen krieg elend flchten masse...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                     article_id  \\\n",
       "5913  FALTER_201501144F8F9DDBEE   \n",
       "\n",
       "                                                  title  \\\n",
       "5913  Mit einem Flüchtling in der WG: Neue Initiativ...   \n",
       "\n",
       "                                             paragraphs  ressort  \\\n",
       "5913  ['Warum müssen Menschen, die vor Krieg und Ele...  Politik   \n",
       "\n",
       "                     authors                       date  \\\n",
       "5913  benedikt narodoslawsky  2015-01-14 00:00:00+00:00   \n",
       "\n",
       "                                           text_cleaned  \n",
       "5913  warum mssen menschen krieg elend flchten masse...  "
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "demo = pd.read_csv(\"../data/cleaned_text.csv\", index_col=False).drop(\"Unnamed: 0\", axis = 1)\n",
    "demo[demo.article_id==\"FALTER_201501144F8F9DDBEE\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "812a4f55-3213-4c2a-bc69-109bbf4b46e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_category(text):\n",
    "    answer = openai.Completion.create(\n",
    "    model=\"ada:ft-became-ai-2023-07-05-14-28-32\",\n",
    "    prompt=text)\n",
    "\n",
    "    print(answer['choices'][0]['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ae67a819-6d13-41c8-a088-f820d8ae6550",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import string\n",
    "\n",
    "def clean_text(text):\n",
    "    # Convert text to lowercase\n",
    "    text = text.lower()\n",
    "\n",
    "    # Remove numbers\n",
    "    text = re.sub(r'\\d+', '', text)\n",
    "\n",
    "    # Remove punctuation\n",
    "    text = text.translate(str.maketrans('', '', string.punctuation))\n",
    "\n",
    "    # Remove whitespace\n",
    "    text = text.strip()\n",
    "\n",
    "    # Remove extra spaces\n",
    "    text = re.sub(' +', ' ', text)\n",
    "\n",
    "    # Remove stopwords (optional)\n",
    "    # stopwords = ['the', 'and', 'in', ...]\n",
    "    # text = ' '.join(word for word in text.split() if word not in stopwords)\n",
    "\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "20256dc3-2c0d-4515-a325-ed94209b9487",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>article_id</th>\n",
       "      <th>title</th>\n",
       "      <th>paragraphs</th>\n",
       "      <th>ressort</th>\n",
       "      <th>authors</th>\n",
       "      <th>date</th>\n",
       "      <th>text_cleaned</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2069</th>\n",
       "      <td>FALTER_201509025E856609CE</td>\n",
       "      <td>Urbanes Betragen</td>\n",
       "      <td>['Frau P. möchte wissen, ob sie im Freiluftkin...</td>\n",
       "      <td>Stadtleben</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2015-09-02 00:00:00+00:00</td>\n",
       "      <td>frau p mchte wissen freiluftkino guten gewisse...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                     article_id             title  \\\n",
       "2069  FALTER_201509025E856609CE  Urbanes Betragen   \n",
       "\n",
       "                                             paragraphs     ressort authors  \\\n",
       "2069  ['Frau P. möchte wissen, ob sie im Freiluftkin...  Stadtleben     NaN   \n",
       "\n",
       "                           date  \\\n",
       "2069  2015-09-02 00:00:00+00:00   \n",
       "\n",
       "                                           text_cleaned  \n",
       "2069  frau p mchte wissen freiluftkino guten gewisse...  "
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "base_article = 'FALTER_201509025E856609CE' # => index 4018\n",
    "text1 = demo[demo.article_id==base_article].text_cleaned.values[0]\n",
    "demo[demo.article_id==base_article]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "7361835f-bd68-4812-a0fc-93e4e5cc39be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " gefreudet ber,denkstleister entstanden\n",
      "\n"
     ]
    }
   ],
   "source": [
    "get_category(text1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "59fe1967-4358-417f-9551-59f651df1305",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Series([], )'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "article = demo[demo.title.str.contains(\"Der Bub, der\")].article_id.to_string(index=False)\n",
    "\n",
    "article = demo[demo.text_cleaned.str.contains(\"gefhlschwer\")].article_id.to_string(index=False)\n",
    "\n",
    "demo[demo.text_cleaned.str.contains(\"gefhlschwer\")].article_id.to_string(index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "d2fc122c-f8fb-4e7f-9667-539088284304",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([\"['Frau P. möchte wissen, ob sie im Freiluftkino guten Gewissens selbst mitgebrachten Proviant jausnen darf?', 'Sie müssen wohl im falschen Film gelandet sein, werte Frau P.! Entscheiden Sie sich: Kino oder Picknick. Denn es ist definitiv nicht die Tatsache, dass Sie Ihre Jause mitbringen und damit andere um Einkünfte umfallen, die Ihnen ein schlechtes Gewissen machen sollte. Vielmehr geht es hier um etwas viel Entscheidenderes: Nebengeräusche. Mit Sackerln raschelnde und laut vor sich hinknuspernde Sitznachbarn im Kino sind die Pest! Auch dass es sich um ein Freiluftkino handelt, macht die Sache nicht wirklich besser, denn dort muss man ohnehin schon gegen eine ziemlich dichte, gegebene Geräuschkulisse ankämpfen. Deshalb, Frau P., haben Sie ein Einsehen und speisen Sie vor oder nach dem Film - Sie werden in den zwei Stunden ohne Nahrungszuführung schon nicht umkommen.', 'Noch Fragen? stadtleben@falter.at']\"],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ger[ger.article_id=='FALTER_201509025E856609CE'][\"paragraphs\"].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "c837ed4b-095e-405a-854e-bebbe6106185",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['Das Geister- schloss der Republik'], dtype=object)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ger[ger.article_id=='FALTER_201501288B07118AE7'][\"title\"].values"
   ]
  },
  {
   "cell_type": "raw",
   "id": "7501a8f8-76aa-43e0-ba4f-f176de481e75",
   "metadata": {},
   "source": [
    "ger[ger.article_id=='FALTER_201501288B07118AE7'][\"paragraphs\"].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "e0c96642-88b0-426e-a9de-de468acad9b1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['[\\'Der Spatenstich war für Jänner 2015 vorgesehen, die feierliche Eröffnung des Weltmuseums Wien für Ende 2016 angekündigt. Stattdessen warnen nun Schilder: Achtung, Sperrzone! Und Direktor Steven Engelsman wendet sich auf der Website mit einer Videobotschaft an das Publikum. Die Ausschreibung der Bauarbeiten sei bereits vorbereitet gewesen, da habe Minister Josef Ostermayer (SPÖ) die Stopptaste gedrückt. \"Der Minister sagt: Das Weltmuseum Wien ist ihm zu groß.\"\\', \\'Was sich bereits im Dezember abzeichnete, ist nun fix. Das ehemalige Museum für Völkerkunde (MVK) im rechten Flügel der Neuen Hofburg wird nicht in der geplanten Form realisiert. Im Winter 2013 hieß es, dass für die Neuaufstellung der ethnografischen Sammlungen auf insgesamt 4600 Quadratmetern staatliche Gelder in der Höhe von 25 Millionen Euro zur Verfügung stehen würden. Plötzlich gibt es nur mehr 16,6 Millionen Euro. Und die Ausstellungsfläche ist auf 3900 Quadratmeter geschrumpft. Die eingesparten Räume und Millionen sollen einem neuen historischen Museum zugutekommen.\\', \\'Museumsmitarbeiter und wissenschaftliche Community sind empört. \"Aus dem Rückenwind wurde Gegenwind\", sagt Direktor Engelsman. Andre Gingrich, einer der wichtigsten Ethnologen des Landes, spricht von einem Pfusch. \"Man gibt uns das Gefühl: Eure Anliegen sind uns wurscht.\" Der grüne Kultursprecher Wolfgang Zinggl nennt Ostermayers Vorgehen eine \"kulturpolitische Schande\".\\', \\'Während die Kunstmuseen expandierten, werde ausgerechnet jenes Projekt zusammengestrichen, das sich mit der multikulturellen Gegenwart auseinandersetze. Südamerikaner etwa pilgern in das Weltmuseum, weil sich hier die sogenannte Federkrone Moctezumas, eines der wichtigsten Zeugnisse der präkolumbianischen Zeit, befindet. Das wiedereröffnete Museum sollte eine Art Haus der Kulturen sein, in dem auch die vielen Migrantinnen und Migranten ihre Ursprünge kennenlernen können.\\', \\'Nur eine Handvoll Museen weltweit verfügt über einen vergleichbaren Bestand. Während sich andere Nationen teure Neubauten für die Darstellung kultureller Diversität leisten, etwa das Musée du quai Branly in Paris, macht Österreich sein einziges Fenster zur Welt ein Stück weit zu.\\', \\'Wer den Heldenplatz betritt, blickt auf einen Monsterbau aus den letzten Jahren der Monarchie. Abweisend ragt die von Heldenfiguren und Säulen rhythmisierte Fassade aus istrischem Marzana-Sandstein in die Höhe, das Antlitz kalter Herrschaftsarchitektur. Die Neue Hofburg war ursprünglich als linker Flügel des sogenannten Kaiserforums geplant, in dem die Herrscherfamilie wohnen (\"Corps de Logis\") und Teile ihrer Sammlungen unterbringen wollte (\"Corps de Musée\"). Heute befinden sich hier die Nationalbibliothek und mehrere Abteilungen des Kunsthistorischen Museums.\\', \\'Die Errichtung verschlang Unsummen, verschliss mehrere Architekten, und als nach dem Ersten Weltkrieg die Republik den schon damals ungeliebten Prunkbau der Habsburger übernahm, war es eine riesige, nicht ganz fertige Hülle für nichts. Weder hatte hier ein Mitglied des Kaiserhauses gewohnt, noch war Bedarf da für ein gigantisches Treppenhaus, endlose Raumfluchten und einen von einem Triumphbogen überwölbten Monumentalbalkon. Bis die Nazis kamen: Adolf Hitler stand am 15. März 1938 auf dem Balkon und verkündete vor einer begeisterten Menge den Anschluss Österreichs an das Deutsche Reich.\\', \\'Im Sommer 1938 zog das \"Zentraldepot beschlagnahmter Kunstwerke\" ein. 8000 von den Nazis geraubte Kulturgüter\\', \\'aus jüdischem Besitz wurden hier gelagert. Es ist, als würden sich die Schatten der Vergangenheit wie ein schwerer Mantel über diesen Ort legen.\\', \\'Als die Republik in den 1990er-Jahren begann, ihre maroden Kulturbauten zu sanieren, blieb die Neue Burg links liegen. Kaum ein Wiener kannte die Sammlungen, die im zentralen Trakt untergebracht sind. Vom Haupteingang gelangt man geradewegs in die Nationalbibliothek. Im Foyer rechter Hand liegt der Eingang zu den Außenstellen des Kunsthistorischen.\\', \\'Über die Prunktreppe hinauf geht es in die oberen Etagen, wo Ritterrüstungen, alte Musikinstrumente und antike Skulpturen ausgestellt sind. Dass es sich dabei um teilweise einzigartige Objekte handelt, wissen nur Fachleute. Kein Plakat wirbt für Mozarts Klavier oder den Reiterharnisch von Kaiser Maximilian.\\', \\'Für die Wiener Bevölkerung war die Neue Burg in erster Linie der Ort, wo man mit den Kindern hingeht, um Indianer und Eskimos zu schauen. Seit den 1920er-Jahren hatte das Museum für Völkerkunde (MVK) seinen Sitz im rechten Trakt, für den sich der Name Corps de Logis eingebürgert hat. Dass sich nun kaum einer über die von Minister Ostermayer verkündete Verkleinerung des Museums aufregt, hat einen einfachen Grund.\\', \\'Die jüngere Generation kennt das Museum gar nicht mehr. Seit 15 Jahren ist es geschlossen oder nur teilweise geöffnet. Ende der 90er-Jahre wurde das Gebäude von Grund auf saniert, für die Einrichtung fehlte dann aber das Geld. Die Umbenennung des MVK in das politisch korrekte \"Weltmuseum\" stiftete zusätzliche Verwirrung.\\', \\'Die schwarz-blaue Regierung beschloss im Jahr 2001 die Eingliederung des MVK - und des Theatermuseums -in den Museumskonzern des Kunsthistorischen; den Steuerzahlern verkaufte Wilfried Seipel, damals KHM-General, das als wirtschaftliche Synergie. \"Dabei ging es nur darum, mit der Basisabgeltung das eigene Budgetloch zu stopfen\", sagt Erwin Melchardt, der sich als ehemaliger Krone-Journalist mit dem Fall beschäftigte und ein Experte für außereuropäische Kunst ist.\\', \\'Die \"feindliche Übernahme\" war auf vier Jahre geplant, doch dann wollten weder das Ministerium noch die KHM-Leitung noch etwas von einer Wiederausgliederung wissen. Das KHM kassierte die für das MVK vorgesehene Basisabgeltung von 4,18 Millionen und zeigte kein Interesse an einem Vollbetrieb; ein geschlossenes Museum kostet weniger. Die Einrichtung der Schauräume wurde mit dem Vorwand verschoben, es gäbe Wichtigeres.\\', \\'So machte Ostermayers Vorgängerin Claudia Schmied (SPÖ) 15 Millionen Euro für die Renovierung der Kunstkammer des KHM locker. Als der damalige MVK-Direktor Christian F. Feest im Jahr 2010 seine \"Sorge um die Zukunft des Hauses\" ausdrückte, kannte die KHM-Generalin Sabine Haag keine Gnade. Feest musste gehen, und die MVK-Mitarbeiter sind seither für die Presse nicht mehr ansprechbar.\\', \\'Ostermayers Sparmaßnahme fallen mehrere südseitig, in Richtung Burggarten, gelegene Flächen zum Opfer. Hier war eine Art Schaulager vorgesehen, ein Einblick in die riesigen Depotbestände. Ebenfalls gestrichen ist eine Außenstelle des Zoom Kindermuseums, das jungen Menschen den Wert fremder Kulturen vermitteln sollte.\\', \\'Dass das fertige Paket wieder aufgeschnürt wird, hat mit einem anderen, in der Regierungserklärung formulierten Vorhaben zu tun, dem Haus der Geschichte Österreichs. Seit Jahrzehnten angestrebt, soll das historische Museum über die Zeit von der Revolution 1848 bis zur Gegenwart nun in der Neuen Burg Wirklichkeit werden.\\', \\'Der Plan sieht 3000 Quadratmeter Ausstellungsfläche vor. Es gibt noch kein Konzept, auch eine Sammlung fehlt. Die Zeit drängt, denn im Jahr 2018 steht ein Gedenkjahr an, mit der Revolution 1848 und der Republiksgründung 1918 als Jubiläen. Unklar ist, wer die Organisation übernehmen wird.\\', \\'Viele Konzepte wurden bereits geschrieben, deren Umsetzung allesamt am Streit der Parteien über die großen historischen Ereignisse scheiterten. So sehen Konservative etwa in Bundeskanzler Engelbert Dollfuß (1892-1934) ein Opfer des Nationalsozialismus, während die Sozialdemokratie ihn für einen faschistischen Diktator hält.\\', \\'Nun zeichnet sich ein historischer Kompromiss ab. Die schwarze Reichshälfte stellt sich im Landesmuseum Niederösterreich in St. Pölten dar, die Roten entfalten in der Habsburgerburg ihre Sicht der Dinge. Landeshauptmann Erwin Pröll (ÖVP) will 2018 ebenfalls gedenken, in einem eigenen \"Haus der Geschichte\". Der progressive Historiker Oliver Rathkolb leitet das Wiener Haus, sein konservativer Grazer Kollege Stefan Karner das Haus in St. Pölten. Zusammenarbeit gibt es keine. \"Ich kenne das Wiener Projekt nur aus der Zeitung\", wundert sich Karner.\\', \\'\"Es gibt überhaupt keinen Bedarf für so ein Museum\", urteilt die Historikerin Eva Blimlinger, Rektorin der Akademie der bildenden Künste. \"Die Beletage in der Hofburg ist der beste Ort um die jüngere Geschichte Österreichs zu vermitteln\", verteidigt Ostermayer sein Haus. Ebenso zahlreich wie die in der Vergangenheit entwickelten Konzepte sind die diskutierten Standorte. Dazu zählten etwas das Palais Epstein, das Heeresgeschichtliche Museum und der Heldenplatz, wo allerdings ein Neubau überlegt wurde. Die Neue Burg ist billiger.\\', \\'Die Ausstellung soll in den historisch brenzligen Räumen untergebracht werden. Der \"Führer-Balkon\" gehört dazu und die angrenzenden Säle der Sammlung alter Musikinstrumente, wo sich die Raubkunstdepots befanden. Durch die neue Nutzung wird die chronologische Reihung der musikalischen Epoche unterbrochen, und auch die Hofjagd-und Rüstkammer des KHM muss tapfer sein. \"Wenn das von höherer Stelle entschieden wird, müssen wir Flächen hergeben\", sagt Sabine Haag.\\', \\'Die Burg ist wie ein vertikales Labyrinth mit unterbrochenen Verbindungsgängen und verschlungenen Raumfolgen, sodass eine sinnvolle Struktur kaum möglich ist. Wandern die Musikinstrumente in die frei werdenden Flächen des Weltmuseums? Wird das Weltmuseum nun zwei Eingänge haben? Bisher gibt es nur Bleistiftskizzen.\\', \\'Als die größenwahnsinnige Neue Burg 1906 nach 25-jähriger Planungs-und Bauzeit noch immer nicht fertig war, gab es in der Wiener Presse eine Flut von Polemiken. Das \"monströse Bauwerk\" wurde als \"Schande für Wien\" bezeichnet, \"der ganze Bau verdient, dem Erdboden gleich gemacht zu werden\". Mit der Verkleinerung des Weltmuseums und dem dilettantischen Start des Hauses der Geschichte wird das Spukschloss nicht einladender. F\\', \\'1 Haus der Geschichte: soll im ersten Obergeschoß der Neuen Burg entstehen\\', \\'2 Ephesos Museum: gehört zum KHM und breitet sich auf der Prachtstiege aus\\', \\'3 Sammlung alter Musikinstrumente: muss Säle für das Haus der Geschichte hergeben\\', \\'4 Hofjagd-und Rüstkammer: die älteste und tollste Rittersammlung der Welt\\', \\'5 Weltmuseum Wien: Das ehemalige Museum für Völkerkunde wird kleiner und billiger\\', \\'6 Nationalbibliothek: ist in die Planung des neuen Hauses der Geschichte mit eingebunden\\']'],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ger[ger.article_id=='FALTER_201501288B07118AE7'][\"paragraphs\"].values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48f6a6ae-9e49-431b-a878-7e85fadadfeb",
   "metadata": {},
   "source": [
    "Those two articles are from 0 to 10 SIMILAR"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
